{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# basic libs #\n",
    "##############\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from __future__ import print_function\n",
    "import os, dicom, sys, csv\n",
    "\n",
    "###########\n",
    "# science #\n",
    "###########\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.misc import imresize\n",
    "from scipy import ndimage\n",
    "\n",
    "#######\n",
    "# ML #\n",
    "######\n",
    "\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.core import Activation, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ken Cavagnolo \n",
      "Last updated: Thu Feb 18 2016 \n",
      "\n",
      "CPython 2.7.10\n",
      "IPython 4.0.3\n",
      "\n",
      "numpy 1.10.4\n",
      "scipy 0.17.0\n",
      "keras 0.3.2\n",
      "theano 0.8.0.dev0\n",
      "scikit-learn 0.17\n",
      "scikit-image 0.11.3\n",
      "\n",
      "compiler   : GCC 5.2.1 20151010\n",
      "system     : Linux\n",
      "release    : 4.2.0-23-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : ubuntu\n",
      "Git hash   : d789250ba37d65094a9a2958979a70b6344f7cba\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Ken Cavagnolo\" -n -u -v -m -h -g -p numpy,scipy,keras,theano,scikit-learn,scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crps(true, pred):\n",
    "    return np.sum(np.square(true - pred)) / true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_to_cdf(y, sigma=1e-10):\n",
    "    cdf = np.zeros((y.shape[0], 600))\n",
    "    for i in range(y.shape[0]):\n",
    "        cdf[i] = norm.cdf(np.linspace(0, 599, 600), y[i], sigma)\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    progbar = Progbar(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X[i, j] = denoise_tv_chambolle(X[i, j], weight=0.1, multichannel=False)\n",
    "        progbar.add(1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotation_augmentation(X, angle_range):\n",
    "    progbar = Progbar(X.shape[0])\n",
    "    X_rot = np.copy(X)\n",
    "    for i in range(len(X)):\n",
    "        angle = np.random.randint(-angle_range, angle_range)\n",
    "        for j in range(X.shape[1]):\n",
    "            X_rot[i, j] = ndimage.rotate(X[i, j], angle, reshape=False, order=1)\n",
    "        progbar.add(1)\n",
    "    return X_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift_augmentation(X, h_range, w_range):\n",
    "    progbar = Progbar(X.shape[0])\n",
    "    X_shift = np.copy(X)\n",
    "    size = X.shape[2:]\n",
    "    for i in range(len(X)):\n",
    "        h_random = np.random.rand() * h_range * 2. - h_range\n",
    "        w_random = np.random.rand() * w_range * 2. - w_range\n",
    "        h_shift = int(h_random * size[0])\n",
    "        w_shift = int(w_random * size[1])\n",
    "        for j in range(X.shape[1]):\n",
    "            X_shift[i, j] = ndimage.shift(X[i, j], (h_shift, w_shift), order=0)\n",
    "        progbar.add(1)\n",
    "    return X_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_resize(img):\n",
    "    if img.shape[0] < img.shape[1]:\n",
    "        img = img.T\n",
    "    short_edge = min(img.shape[:2])\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    img = crop_img\n",
    "    img = imresize(img, img_shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(from_dir, verbose=True):\n",
    "    print('-'*50)\n",
    "    print('Loading all DICOM images from {0}...'.format(from_dir))\n",
    "    print('-'*50)\n",
    "\n",
    "    current_study_sub = ''  # saves the current study sub_folder\n",
    "    current_study = ''  # saves the current study folder\n",
    "    current_study_images = []  # holds current study images\n",
    "    ids = []  # keeps the ids of the studies\n",
    "    study_to_images = dict()  # dictionary for studies to images\n",
    "    total = 0\n",
    "    images = []  # saves 30-frame-images\n",
    "    \n",
    "    from_dir = from_dir if from_dir.endswith('/') else from_dir + '/'\n",
    "    for subdir, _, files in os.walk(from_dir):\n",
    "        subdir = subdir.replace('\\\\', '/')  # windows path fix\n",
    "        subdir_split = subdir.split('/')\n",
    "        study_id = subdir_split[-3]\n",
    "        if \"sax\" in subdir:\n",
    "            for f in files:\n",
    "                image_path = os.path.join(subdir, f)\n",
    "                if not image_path.endswith('.dcm'):\n",
    "                    continue\n",
    "\n",
    "                image = dicom.read_file(image_path)\n",
    "                image = image.pixel_array.astype(float)\n",
    "                image /= np.max(image)  # scale to [0,1]\n",
    "                if img_resize:\n",
    "                    image = crop_resize(image)\n",
    "\n",
    "                if current_study_sub != subdir:\n",
    "                    x = 0\n",
    "                    try:\n",
    "                        while len(images) < 30:\n",
    "                            images.append(images[x])\n",
    "                            x += 1\n",
    "                        if len(images) > 30:\n",
    "                            images = images[0:30]\n",
    "\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    current_study_sub = subdir\n",
    "                    current_study_images.append(images)\n",
    "                    images = []\n",
    "\n",
    "                if current_study != study_id:\n",
    "                    study_to_images[current_study] = np.array(current_study_images)\n",
    "                    if current_study != \"\":\n",
    "                        ids.append(current_study)\n",
    "                    current_study = study_id\n",
    "                    current_study_images = []\n",
    "                images.append(image)\n",
    "                if verbose:\n",
    "                    if total % 1000 == 0:\n",
    "                        print('Images processed {0}'.format(total))\n",
    "                total += 1\n",
    "    x = 0\n",
    "    try:\n",
    "        while len(images) < 30:\n",
    "            images.append(images[x])\n",
    "            x += 1\n",
    "        if len(images) > 30:\n",
    "            images = images[0:30]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    print('-'*50)\n",
    "    print('All DICOM in {0} images loaded.'.format(from_dir))\n",
    "    print('-'*50)\n",
    "\n",
    "    current_study_images.append(images)\n",
    "    study_to_images[current_study] = np.array(current_study_images)\n",
    "    if current_study != \"\":\n",
    "        ids.append(current_study)\n",
    "\n",
    "    return ids, study_to_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_studies_results():\n",
    "    id_to_results = dict()\n",
    "    train_csv = open('data/train.csv')\n",
    "    lines = train_csv.readlines()\n",
    "    i = 0\n",
    "    for item in lines:\n",
    "        if i == 0:\n",
    "            i = 1\n",
    "            continue\n",
    "        id, diastole, systole = item.replace('\\n', '').split(',')\n",
    "        id_to_results[id] = [float(diastole), float(systole)]\n",
    "    return id_to_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_train_npy():\n",
    "    print('-'*50)\n",
    "    print('Writing training data to .npy file...')\n",
    "    print('-'*50)\n",
    "\n",
    "    study_ids, images = load_images('data/train')  # load images and their ids\n",
    "    studies_to_results = map_studies_results()  # load the dictionary of studies to targets\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for study_id in study_ids:\n",
    "        study = images[study_id]\n",
    "        outputs = studies_to_results[study_id]\n",
    "        for i in range(study.shape[0]):\n",
    "            X.append(study[i, :, :, :])\n",
    "            y.append(outputs)\n",
    "\n",
    "    X = np.array(X, dtype=np.uint8)\n",
    "    y = np.array(y)\n",
    "    np.save('data/X_train.npy', X)\n",
    "    np.save('data/y_train.npy', y)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_validation_npy():\n",
    "    print('-'*50)\n",
    "    print('Writing validation data to .npy file...')\n",
    "    print('-'*50)\n",
    "\n",
    "    ids, images = load_images('data/validate')\n",
    "    study_ids = []\n",
    "    X = []\n",
    "\n",
    "    for study_id in ids:\n",
    "        study = images[study_id]\n",
    "        for i in range(study.shape[0]):\n",
    "            study_ids.append(study_id)\n",
    "            X.append(study[i, :, :, :])\n",
    "\n",
    "    X = np.array(X, dtype=np.uint8)\n",
    "    np.save('data/X_validate.npy', X)\n",
    "    np.save('data/ids_validate.npy', study_ids)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Writing training data to .npy file...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Loading all DICOM images from data/train...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "All DICOM in data/train/ images loaded.\n",
      "--------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------\n",
      "Writing validation data to .npy file...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Loading all DICOM images from data/validate...\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "All DICOM in data/validate/ images loaded.\n",
      "--------------------------------------------------\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "img_resize = True\n",
    "img_shape = (64, 64)\n",
    "\n",
    "write_train_npy()\n",
    "write_validation_npy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_normalize(x):\n",
    "    return (x - K.mean(x)) / K.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Activation(activation=center_normalize, input_shape=(30, 64, 64)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(96, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(96, 3, 3, border_mode='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(128, 2, 2, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(128, 2, 2, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, W_regularizer=l2(1e-3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=root_mean_squared_error)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    X = np.load('data/X_train.npy')\n",
    "    y = np.load('data/y_train.npy')\n",
    "    X = X.astype(np.float32)\n",
    "    X /= 255\n",
    "    seed = np.random.randint(1, 10e6)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(X, y, split_ratio=0.2):\n",
    "    split = X.shape[0] * split_ratio\n",
    "    X_test = X[:split, :, :, :]\n",
    "    y_test = y[:split, :]\n",
    "    X_train = X[split:, :, :, :]\n",
    "    y_train = y[split:, :]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('Loading and compiling models...')\n",
    "    model_systole = get_model()\n",
    "    model_diastole = get_model()\n",
    "\n",
    "    print('Loading training data...')\n",
    "    X, y = load_train_data()\n",
    "\n",
    "    print('Pre-processing images...')\n",
    "    X = preprocess(X)\n",
    "\n",
    "    # split to training and test\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, split_ratio=0.2)\n",
    "\n",
    "    nb_iter = 200\n",
    "    epochs_per_iter = 1\n",
    "    batch_size = 32\n",
    "    calc_crps = 1\n",
    "\n",
    "    # remember min val. losses (best iterations), used as sigmas for submission\n",
    "    min_val_loss_systole = sys.float_info.max\n",
    "    min_val_loss_diastole = sys.float_info.max\n",
    "\n",
    "    print('-'*50)\n",
    "    print('Training...')\n",
    "    print('-'*50)\n",
    "\n",
    "    for i in range(nb_iter):\n",
    "        print('-'*50)\n",
    "        print('Iteration {0}/{1}'.format(i + 1, nb_iter))\n",
    "        print('-'*50)\n",
    "\n",
    "        print('Augmenting images - rotations')\n",
    "        X_train_aug = rotation_augmentation(X_train, 15)\n",
    "        print('Augmenting images - shifts')\n",
    "        X_train_aug = shift_augmentation(X_train_aug, 0.1, 0.1)\n",
    "\n",
    "        print('Fitting systole model...')\n",
    "        hist_systole = model_systole.fit(X_train_aug, y_train[:, 0], shuffle=True, nb_epoch=epochs_per_iter,\n",
    "                                         batch_size=batch_size, validation_data=(X_test, y_test[:, 0]))\n",
    "\n",
    "        print('Fitting diastole model...')\n",
    "        hist_diastole = model_diastole.fit(X_train_aug, y_train[:, 1], shuffle=True, nb_epoch=epochs_per_iter,\n",
    "                                           batch_size=batch_size, validation_data=(X_test, y_test[:, 1]))\n",
    "\n",
    "        # sigmas for predicted data, actually loss function values (RMSE)\n",
    "        loss_systole = hist_systole.history['loss'][-1]\n",
    "        loss_diastole = hist_diastole.history['loss'][-1]\n",
    "        val_loss_systole = hist_systole.history['val_loss'][-1]\n",
    "        val_loss_diastole = hist_diastole.history['val_loss'][-1]\n",
    "\n",
    "        if calc_crps > 0 and i % calc_crps == 0:\n",
    "            print('Evaluating CRPS...')\n",
    "            pred_systole = model_systole.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "            pred_diastole = model_diastole.predict(X_train, batch_size=batch_size, verbose=1)\n",
    "            val_pred_systole = model_systole.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "            val_pred_diastole = model_diastole.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "            # CDF for train and test data (actually a step function)\n",
    "            cdf_train = real_to_cdf(np.concatenate((y_train[:, 0], y_train[:, 1])))\n",
    "            cdf_test = real_to_cdf(np.concatenate((y_test[:, 0], y_test[:, 1])))\n",
    "\n",
    "            # CDF for predicted data\n",
    "            cdf_pred_systole = real_to_cdf(pred_systole, loss_systole)\n",
    "            cdf_pred_diastole = real_to_cdf(pred_diastole, loss_diastole)\n",
    "            cdf_val_pred_systole = real_to_cdf(val_pred_systole, val_loss_systole)\n",
    "            cdf_val_pred_diastole = real_to_cdf(val_pred_diastole, val_loss_diastole)\n",
    "\n",
    "            # evaluate CRPS on training data\n",
    "            crps_train = crps(cdf_train, np.concatenate((cdf_pred_systole, cdf_pred_diastole)))\n",
    "            print('CRPS(train) = {0}'.format(crps_train))\n",
    "\n",
    "            # evaluate CRPS on test data\n",
    "            crps_test = crps(cdf_test, np.concatenate((cdf_val_pred_systole, cdf_val_pred_diastole)))\n",
    "            print('CRPS(test) = {0}'.format(crps_test))\n",
    "\n",
    "        print('Saving weights...')\n",
    "        # save weights so they can be loaded later\n",
    "        model_systole.save_weights('weights_systole.hdf5', overwrite=True)\n",
    "        model_diastole.save_weights('weights_diastole.hdf5', overwrite=True)\n",
    "\n",
    "        # for best (lowest) val losses, save weights\n",
    "        if val_loss_systole < min_val_loss_systole:\n",
    "            min_val_loss_systole = val_loss_systole\n",
    "            model_systole.save_weights('weights_systole_best.hdf5', overwrite=True)\n",
    "\n",
    "        if val_loss_diastole < min_val_loss_diastole:\n",
    "            min_val_loss_diastole = val_loss_diastole\n",
    "            model_diastole.save_weights('weights_diastole_best.hdf5', overwrite=True)\n",
    "\n",
    "        # save best (lowest) val losses in file (to be later used for generating submission)\n",
    "        with open('val_loss.txt', mode='w+') as f:\n",
    "            f.write(str(min_val_loss_systole))\n",
    "            f.write('\\n')\n",
    "            f.write(str(min_val_loss_diastole))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_validation_data():\n",
    "    X = np.load('data/X_validate.npy')\n",
    "    ids = np.load('data/ids_validate.npy')\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    X /= 255\n",
    "\n",
    "    return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accumulate_study_results(ids, prob):\n",
    "    sum_result = {}\n",
    "    cnt_result = {}\n",
    "    size = prob.shape[0]\n",
    "    for i in range(size):\n",
    "        study_id = ids[i]\n",
    "        idx = int(study_id)\n",
    "        if idx not in cnt_result:\n",
    "            cnt_result[idx] = 0.\n",
    "            sum_result[idx] = np.zeros((1, prob.shape[1]), dtype=np.float32)\n",
    "        cnt_result[idx] += 1\n",
    "        sum_result[idx] += prob[i, :]\n",
    "    for i in cnt_result.keys():\n",
    "        sum_result[i][:] /= cnt_result[i]\n",
    "    return sum_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission():\n",
    "    print('Loading and compiling models...')\n",
    "    model_systole = get_model()\n",
    "    model_diastole = get_model()\n",
    "\n",
    "    print('Loading models weights...')\n",
    "    model_systole.load_weights('weights_systole_best.hdf5')\n",
    "    model_diastole.load_weights('weights_diastole_best.hdf5')\n",
    "\n",
    "    # load val losses to use as sigmas for CDF\n",
    "    with open('val_loss.txt', mode='r') as f:\n",
    "        val_loss_systole = float(f.readline())\n",
    "        val_loss_diastole = float(f.readline())\n",
    "\n",
    "    print('Loading validation data...')\n",
    "    X, ids = load_validation_data()\n",
    "\n",
    "    print('Pre-processing images...')\n",
    "    X = preprocess(X)\n",
    "\n",
    "    batch_size = 32\n",
    "    print('Predicting on validation data...')\n",
    "    pred_systole = model_systole.predict(X, batch_size=batch_size, verbose=1)\n",
    "    pred_diastole = model_diastole.predict(X, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # real predictions to CDF\n",
    "    cdf_pred_systole = real_to_cdf(pred_systole, val_loss_systole)\n",
    "    cdf_pred_diastole = real_to_cdf(pred_diastole, val_loss_diastole)\n",
    "\n",
    "    print('Accumulating results...')\n",
    "    sub_systole = accumulate_study_results(ids, cdf_pred_systole)\n",
    "    sub_diastole = accumulate_study_results(ids, cdf_pred_diastole)\n",
    "\n",
    "    # write to submission file\n",
    "    print('Writing submission to file...')\n",
    "    fi = csv.reader(open('data/sample_submission_validate.csv'))\n",
    "    f = open('submission.csv', 'w')\n",
    "    fo = csv.writer(f, lineterminator='\\n')\n",
    "    fo.writerow(fi.next())\n",
    "    for line in fi:\n",
    "        idx = line[0]\n",
    "        key, target = idx.split('_')\n",
    "        key = int(key)\n",
    "        out = [idx]\n",
    "        if key in sub_systole:\n",
    "            if target == 'Diastole':\n",
    "                out.extend(list(sub_diastole[key][0]))\n",
    "            else:\n",
    "                out.extend(list(sub_systole[key][0]))\n",
    "        else:\n",
    "            print('Miss {0}'.format(idx))\n",
    "        fo.writerow(out)\n",
    "    f.close()\n",
    "\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS GPU Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [AWS console](https://console.aws.amazon.com)\n",
    "* [A how-to guide here](http://markus.com/install-theano-on-aws)\n",
    "* [Some optimizations](http://techblog.netflix.com/2014/02/distributed-neural-networks-with-gpus.html)\n",
    "* [64-bit cuda Ubuntu repo](http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64)\n",
    "\n",
    "ID: i-481b01cd\n",
    "\n",
    "ZN: us-east-1b\n",
    "\n",
    "DI: i-481b01cd:/dev/sda1\n",
    "\n",
    "```shell\n",
    "# access instance\n",
    "ssh -i ~/.ssh/aws.pem ubuntu@[DNS]\n",
    "\n",
    "# update instance\n",
    "sudo apt-get update\n",
    "sudo apt-get -y dist-upgrade\n",
    "sudo apt-get install -y gcc g++ gfortran build-essential git wget linux-image-generic libopenblas-dev python-dev python-pip python-nose emacs24 tcsh lynx zip libfreetype6-dev libxft-dev libblas-dev liblapack-dev libatlas-base-dev reptyr screen libhdf5-dev\n",
    "\n",
    "# install cuda to make GPU accel possible\n",
    "sudo wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/cuda-repo-ubuntu1404_7.5-18_amd64.deb\n",
    "sudo dpkg -i cuda-repo-ubuntu1404_7.5-18_amd64.deb\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y cuda\n",
    "\n",
    "# install cudnn; speeds process by ~20%\n",
    "wget https://s3.amazonaws.com/emolson/pydata/cudnn-7.0-linux-x64-v3.0-prod.tgz\n",
    "tar -xzvf cudnn-7.0-linux-x64-v3.0-prod.tgz\n",
    "sudo cp cuda/lib64/* /usr/local/cuda/lib64\n",
    "sudo cp cuda/include/* /usr/local/cuda/include\n",
    "\n",
    "# get cuda into cshrc\n",
    "echo -e \"setenv CUDA_HOME /usr/local/cuda\" >> ~/.cshrc\n",
    "echo -e \"set path=($path $CUDA_HOME/bin)\" >> ~/.cshrc\n",
    "echo -e \"setenv LD_LIBRARY_PATH $CUDA_HOME/lib64\" >> ~/.cshrc\n",
    "\n",
    "# reboot and test\n",
    "sudo reboot\n",
    "ssh -i ~/.ssh/aws.pem ubuntu@[DNS]\n",
    "cuda-install-samples-7.5.sh ~/\n",
    "cd NVIDIA_CUDA-7.5_Samples/1_Utilities/deviceQuery; make; ./deviceQuery\n",
    "\n",
    "# install bleeding edge theano and deps\n",
    "sudo pip install cython h5py pyyaml ipython pydicom\n",
    "sudo pip install --upgrade git+git://github.com/Theano/Theano.git\n",
    "sudo pip install keras\n",
    "\n",
    "# opt theanorc\n",
    "emacs ~/.theanorc\n",
    "\n",
    "[global]\n",
    "floatX = float32\n",
    "device = gpu\n",
    "allow_gc = False\n",
    "[mode] = FAST_RUN\n",
    "optimizer_including = cudnn\n",
    "\n",
    "[lib]\n",
    "cnmem = 0.9\n",
    "\n",
    "[nvcc]\n",
    "fastmath = True\n",
    "\n",
    "[cuda]\n",
    "root = /usr/local/cuda\n",
    "\n",
    "# set cookies for lynx\n",
    "emacs ~/.lynxrc \n",
    "\n",
    "SET_COOKIES:TRUE\n",
    "ACCEPT_ALL_COOKIES:TRUE\n",
    "PERSISTENT_COOKIES:TRUE\n",
    "COOKIE_FILE:~/.lynx_cookies\n",
    "COOKIE_SAVE_FILE:~/.lynx_cookies\n",
    "```\n",
    "\n",
    "**Above improvements took compute time from 3.6 min per iteration to 1.9 min per, a 2x improvement.** Test script is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the gpu is functioning\n",
    "# copy code to test.py and run\n",
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "vlen = 10 * 30 * 768\n",
    "iters = 1000\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get data off Kaggle to AWS:\n",
    "\n",
    "```shell\n",
    "lynx\n",
    "lynx -cfg=~/.lynxrc www.kaggle.com\n",
    "#<navigate to login, check 'Remember me' and exit>\n",
    "mv ~/.lynx_cookies ~/cookies\n",
    "wget -c --load-cookies=/home/ubuntu/cookies https://www.kaggle.com/c/second-annual-data-science-bowl/download/validate.zip\n",
    "wget -c --load-cookies=/home/ubuntu/cookies https://www.kaggle.com/c/second-annual-data-science-bowl/download/train.zip\n",
    "wget -c --load-cookies=/home/ubuntu/cookies https://www.kaggle.com/c/second-annual-data-science-bowl/download/train.csv.zip\n",
    "wget -c --load-cookies=/home/ubuntu/cookies https://www.kaggle.com/c/second-annual-data-science-bowl/download/sample_submission_validate.csv.zip\n",
    "```\n",
    "\n",
    "Run the ml pipeline:\n",
    "```shell\n",
    "screen -S theano\n",
    "tcsh\n",
    "cd kaggle_comp/\n",
    "python data.py\n",
    "python train.py\n",
    "python submission.py\n",
    "```\n",
    "\n",
    "Forget to <code>screen</code> before running the above?\n",
    "```shell\n",
    "ps aux | grep python # note the pid\n",
    "reptyr <pid>\n",
    "```\n",
    "\n",
    "If above throws an operation denied error, run the below and re-try the above:\n",
    "```shell\n",
    "sudo emacs -nw /etc/sysctl.d/10-ptrace.conf\n",
    "# set kernel.yama.ptrace_scope = 0\n",
    "sudo sysctl -p /etc/sysctl.d/10-ptrace.conf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
