{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T16:29:16.708862",
     "start_time": "2016-10-05T16:29:13.867998"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ken Cavagnolo \n",
      "last updated: Wed Oct 05 2016 \n",
      "\n",
      "CPython 2.7.12\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.1\n",
      "scipy 0.18.1\n",
      "pandas 0.19.0\n",
      "sklearn 0.18\n",
      "matplotlib 1.5.3\n",
      "plotly 1.12.9\n",
      "\n",
      "compiler   : GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\n",
      "system     : Linux\n",
      "release    : 4.4.0-38-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : ubuntu\n",
      "Git hash   : 0eced70c6957019dd0556689997e69cf4e5624cf\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# basics #\n",
    "##########\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import collections\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "###########\n",
    "# science #\n",
    "###########\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "rseed = random.seed(42)\n",
    "\n",
    "######\n",
    "# ml #\n",
    "######\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "###################\n",
    "# sklearn tooling #\n",
    "###################\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import grid_search\n",
    "from sklearn import pipeline\n",
    "from sklearn import feature_selection\n",
    "\n",
    "#################\n",
    "# visualization #\n",
    "#################\n",
    "\n",
    "# plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "from plotly.graph_objs import *\n",
    "import cufflinks as cf\n",
    "tls.set_credentials_file(username=os.environ.get('PLOTLY_USERNAME'), api_key=os.environ.get('PLOTLY_APIKEY'))\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='pearl')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "mpl.rcParams['figure.figsize']=(12.0,4.0)\n",
    "%matplotlib inline\n",
    "\n",
    "############\n",
    "# sys info #\n",
    "############\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Ken Cavagnolo\" -n -u -v -m -h -g -p numpy,scipy,pandas,sklearn,\\\n",
    "matplotlib,plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Taken from [here](https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T16:29:21.308645",
     "start_time": "2016-10-05T16:29:21.297654"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# decide on network size\n",
    "n_neurons = 200\n",
    "n_layers = 3\n",
    "dropout = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T16:29:21.591472",
     "start_time": "2016-10-05T16:29:21.585530"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add cell and update itself\n",
    "cell = tf.nn.rnn_cell.GRUCell(n_neurons)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([cell] * n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T16:29:23.223527",
     "start_time": "2016-10-05T16:29:22.897855"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup the time steps\n",
    "max_length = 100\n",
    "data = tf.placeholder(tf.float32, [None, max_length, 28])\n",
    "output, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Complex RNN (char-rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Inspired by [this](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and taken from [here](https://github.com/sherjilozair/char-rnn-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T15:05:46.105147",
     "start_time": "2016-10-05T15:05:45.914728"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "        # When the data (tesor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T15:06:08.353910",
     "start_time": "2016-10-05T15:06:08.323452"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--save_dir', type=str, default='save',\n",
    "                       help='model directory to store checkpointed models')\n",
    "    parser.add_argument('-n', type=int, default=500,\n",
    "                       help='number of characters to sample')\n",
    "    parser.add_argument('--prime', type=text_type, default=u' ',\n",
    "                       help='prime text')\n",
    "    parser.add_argument('--sample', type=int, default=1,\n",
    "                       help='0 to use max at each timestep, 1 to sample at each timestep, 2 to sample on spaces')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    sample(args)\n",
    "\n",
    "def sample(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    model = Model(saved_args, True)\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(model.sample(sess, chars, vocab, args.n, args.prime, args.sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T15:06:12.311636",
     "start_time": "2016-10-05T15:06:12.154290"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import seq2seq\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, args, infer=False):\n",
    "        self.args = args\n",
    "        if infer:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "\n",
    "        if args.model == 'rnn':\n",
    "            cell_fn = rnn_cell.BasicRNNCell\n",
    "        elif args.model == 'gru':\n",
    "            cell_fn = rnn_cell.GRUCell\n",
    "        elif args.model == 'lstm':\n",
    "            cell_fn = rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "        cell = cell_fn(args.rnn_size)\n",
    "\n",
    "        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        outputs, last_state = seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([args.batch_size * args.seq_length])],\n",
    "                args.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = self.cell.zero_state(1, tf.float32).eval()\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-05T15:06:25.453003",
     "start_time": "2016-10-05T15:06:25.302598"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                       help='data directory containing input.txt')\n",
    "    parser.add_argument('--save_dir', type=str, default='save',\n",
    "                       help='directory to store checkpointed models')\n",
    "    parser.add_argument('--rnn_size', type=int, default=128,\n",
    "                       help='size of RNN hidden state')\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                       help='number of layers in the RNN')\n",
    "    parser.add_argument('--model', type=str, default='lstm',\n",
    "                       help='rnn, gru, or lstm')\n",
    "    parser.add_argument('--batch_size', type=int, default=50,\n",
    "                       help='minibatch size')\n",
    "    parser.add_argument('--seq_length', type=int, default=50,\n",
    "                       help='RNN sequence length')\n",
    "    parser.add_argument('--num_epochs', type=int, default=50,\n",
    "                       help='number of epochs')\n",
    "    parser.add_argument('--save_every', type=int, default=1000,\n",
    "                       help='save frequency')\n",
    "    parser.add_argument('--grad_clip', type=float, default=5.,\n",
    "                       help='clip gradients at this value')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.002,\n",
    "                       help='learning rate')\n",
    "    parser.add_argument('--decay_rate', type=float, default=0.97,\n",
    "                       help='decay rate for rmsprop')                       \n",
    "    parser.add_argument('--init_from', type=str, default=None,\n",
    "                       help=\"\"\"continue training from saved model at this path. Path must contain files saved by previous training process: \n",
    "                            'config.pkl'        : configuration;\n",
    "                            'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                            'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                                  Note: this file contains absolute paths, be careful when moving files around;\n",
    "                            'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                        \"\"\")\n",
    "    args = parser.parse_args()\n",
    "    train(args)\n",
    "\n",
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    \n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist \n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.get_checkpoint_state(args.init_from)\n",
    "        assert ckpt,\"No checkpoint found\"\n",
    "        assert ckpt.model_checkpoint_path,\"No model path found in checkpoint\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl')) as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same=[\"model\",\"rnn_size\",\"num_layers\",\"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "        \n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl')) as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "        \n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "        \n",
    "    model = Model(args)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = model.initial_state.eval()\n",
    "            for b in range(data_loader.num_batches):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "                end = time.time()\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                    .format(e * data_loader.num_batches + b,\n",
    "                            args.num_epochs * data_loader.num_batches,\n",
    "                            e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                    or (e==args.num_epochs-1 and b == data_loader.num_batches-1): # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step = e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LSTM w/ GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
