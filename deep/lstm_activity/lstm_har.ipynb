{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LSTM for Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Human activity recognition using smartphones dataset and an LSTM RNN. Classifying the type of movement amongst six categories:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"400\" height=\"300\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I will be using an LSTM on the data to learn (as a cellphone attached on the waist) to recognise the type of activity that the user is doing.\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:56:12.446792",
     "start_time": "2016-10-25T13:56:09.280689"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ken Cavagnolo \n",
      "last updated: Tue Oct 25 2016 \n",
      "\n",
      "CPython 2.7.12\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.1\n",
      "scipy 0.18.1\n",
      "pandas 0.19.0\n",
      "sklearn 0.18\n",
      "theano 0.8.2\n",
      "keras 1.1.0\n",
      "tensorflow 0.10.0rc0\n",
      "matplotlib 1.5.3\n",
      "seaborn 0.7.1\n",
      "plotly 1.12.9\n",
      "\n",
      "compiler   : GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\n",
      "system     : Linux\n",
      "release    : 4.4.0-42-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : ubuntu\n",
      "Git hash   : 1ac763e435ea438d89f382aa559adeb760c6b98e\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# basics #\n",
    "##########\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import collections\n",
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "###########\n",
    "# science #\n",
    "###########\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "rseed = random.seed(42)\n",
    "np.random.seed(rseed)\n",
    "\n",
    "######\n",
    "# ml #\n",
    "######\n",
    "\n",
    "import theano as thno\n",
    "import keras as krs\n",
    "import tensorflow as tf\n",
    "\n",
    "###################\n",
    "# sklearn tooling #\n",
    "###################\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import grid_search\n",
    "from sklearn import pipeline\n",
    "from sklearn import feature_selection\n",
    "\n",
    "#################\n",
    "# visualization #\n",
    "#################\n",
    "\n",
    "# plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "from plotly.graph_objs import *\n",
    "import cufflinks as cf\n",
    "tls.set_credentials_file(username=os.environ.get('PLOTLY_USERNAME'), api_key=os.environ.get('PLOTLY_APIKEY'))\n",
    "cf.set_config_file(offline=False, world_readable=False, theme='pearl')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "mpl.rcParams['figure.figsize']=(12.0,4.0)\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted', n_colors=15, desat=None)\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "############\n",
    "# sys info #\n",
    "############\n",
    "\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Ken Cavagnolo\" -n -u -v -m -h -g -p numpy,scipy,pandas,sklearn,theano,keras,tensorflow,\\\n",
    "matplotlib,seaborn,plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:56:17.147949",
     "start_time": "2016-10-25T13:56:17.143698"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rootpath = '/home/kcavagnolo/ml_fun/deep/lstm_activity/'\n",
    "datadir = 'data/'\n",
    "datapath = datadir + 'UCI HAR Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:56:18.398626",
     "start_time": "2016-10-25T13:56:18.159002"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kcavagnolo/ml_fun/deep/lstm_activity\n",
      "data  lstm_har.ipynb  README.md  UCI HAR Dataset.zip\n",
      "/home/kcavagnolo/ml_fun/deep/lstm_activity/data\n",
      "UCI HAR Dataset\n"
     ]
    }
   ],
   "source": [
    "os.chdir(rootpath)\n",
    "!pwd && ls\n",
    "os.chdir(datadir)\n",
    "!pwd && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:56:48.485982",
     "start_time": "2016-10-25T13:56:26.190700"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "Downloading done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import copy\n",
    "import os\n",
    "from subprocess import call\n",
    "print(\"Downloading...\")\n",
    "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
    "    call(\n",
    "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"Downloading done.\\n\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded. Did not download twice.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:57:07.206072",
     "start_time": "2016-10-25T13:57:06.935174"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n",
      "Extracting successfully done to /home/kcavagnolo/ml_fun/deep/lstm_activity/data/data/UCI HAR Dataset.\n",
      "/home/kcavagnolo/ml_fun/deep/lstm_activity/data\n",
      "__MACOSX  UCI HAR Dataset  UCI HAR Dataset.zip\n",
      "/home/kcavagnolo/ml_fun/deep/lstm_activity\n",
      "data  lstm_har.ipynb  README.md  UCI HAR Dataset.zip\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting...\")\n",
    "extract_directory = os.path.abspath(datapath)\n",
    "if not os.path.exists(extract_directory):\n",
    "    call('unzip -nq \"UCI HAR Dataset.zip\"', shell=True)\n",
    "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
    "else:\n",
    "    print(\"Dataset already extracted. Did not extract twice.\\n\")\n",
    "!pwd && ls\n",
    "os.chdir(rootpath)\n",
    "!pwd && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:55:46.459424",
     "start_time": "2016-10-25T16:55:46.448615"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_X(paths):\n",
    "    X_signals = []\n",
    "    for p in paths:\n",
    "        f = open(p, 'rb')\n",
    "        evt = [np.array(s, dtype=np.float32) for s in [r.replace('  ', ' ').strip().split(' ') for r in f]]\n",
    "        X_signals.append(evt)\n",
    "        f.close()\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:55:46.833019",
     "start_time": "2016-10-25T16:55:46.823295"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load \"y\"\n",
    "def load_y(path):\n",
    "    f = open(path, 'rb')\n",
    "    y_ = np.array([e for e in [r.replace('  ', ' ').strip().split(' ') for r in f]], dtype=np.int32)\n",
    "    f.close()\n",
    "    return y_ - 1 # subtract 1 for 0-based encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:55:52.325139",
     "start_time": "2016-10-25T16:55:47.801161"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get all paths to data files\n",
    "import glob\n",
    "X_train_dir = rootpath + datapath + \"train/Inertial Signals/\"\n",
    "X_train_signals_paths = sorted([f for f in glob.glob(X_train_dir+'*.txt')])\n",
    "X_test_dir = rootpath + datapath + \"test/Inertial Signals/\"\n",
    "X_test_signals_paths = sorted([f for f in glob.glob(X_test_dir+'*.txt')])\n",
    "\n",
    "# load X\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "# load y\n",
    "y_train = load_y(rootpath + datapath + \"train/y_train.txt\")\n",
    "y_test = load_y(rootpath + datapath + \"test/y_test.txt\")\n",
    "\n",
    "# y encodings (found in activity_labels.txt)\n",
    "labels = [\"walk\",\n",
    "          \"walk_up\",\n",
    "          \"walk_down\",\n",
    "          \"sit\",\n",
    "          \"stand\",\n",
    "          \"lay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:56:14.079644",
     "start_time": "2016-10-25T16:56:14.020383"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (7352, 128, 9)\n",
      "Mean, std: 0.102066 0.402165\n",
      "y train shape: (7352, 1)\n",
      "\n",
      "X test shape: (2947, 128, 9)\n",
      "Mean, std: 0.0991399 0.395671\n",
      "y test shape: (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "# data description \n",
    "train_len = len(X_train)\n",
    "test_len = len(X_test)\n",
    "n_steps = len(X_train[0])\n",
    "n_input = len(X_train[0][0])\n",
    "\n",
    "# Some debugging info\n",
    "print \"X train shape:\", X_train.shape\n",
    "print \"Mean, std:\", np.mean(X_train), np.std(X_train)\n",
    "print \"y train shape:\", y_train.shape\n",
    "print \"\"\n",
    "print \"X test shape:\", X_test.shape\n",
    "print \"Mean, std:\", np.mean(X_test), np.std(X_test)\n",
    "print \"y test shape:\", y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:59:35.093881",
     "start_time": "2016-10-25T16:59:35.088031"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:59:36.058967",
     "start_time": "2016-10-25T16:59:36.040537"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _istate, _weights, _biases):\n",
    "    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n",
    "    # Note, some code of this notebook is inspired from an slightly different \n",
    "    # RNN architecture used on another dataset: \n",
    "    # https://tensorhub.com/aymericdamien/tensorflow-rnn\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, n_input]) # (n_steps*batch_size, n_input)\n",
    "    \n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)#, state_is_tuple=True)\n",
    "\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.rnn(lstm_cell, _X, initial_state=_istate)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:59:36.504648",
     "start_time": "2016-10-25T16:59:36.496273"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, step, batch_size):\n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "\n",
    "    return batch_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:59:36.867719",
     "start_time": "2016-10-25T16:59:36.863064"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_):  \n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    return ohe.fit_transform(y_).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:59:37.491732",
     "start_time": "2016-10-25T16:59:37.485893"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lstm params\n",
    "n_hidden = 30\n",
    "n_classes = len(labels)\n",
    "learning_rate = 0.01\n",
    "training_iters = train_len * 100\n",
    "batch_size = 1500\n",
    "display_iter = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T16:59:49.753526",
     "start_time": "2016-10-25T16:59:37.882032"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7efcf8096d90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "istate = tf.placeholder(\"float\", [None, 2*n_hidden]) #state & cell => 2x n_hidden\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, istate, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T17:10:35.692563",
     "start_time": "2016-10-25T17:03:36.173437"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1500, Batch Loss= 4.449417, Accuracy= 0.120666667819\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 2.46767115593, Accuracy= 0.184933826327\n",
      "Iter 15000, Batch Loss= 1.240909, Accuracy= 0.439333319664\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 1.26127171516, Accuracy= 0.439769268036\n",
      "Iter 30000, Batch Loss= 0.986687, Accuracy= 0.574666678905\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 1.03885686398, Accuracy= 0.562606036663\n",
      "Iter 45000, Batch Loss= 0.860126, Accuracy= 0.617999970913\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.920174241066, Accuracy= 0.616219878197\n",
      "Iter 60000, Batch Loss= 0.717557, Accuracy= 0.66333335638\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.772662520409, Accuracy= 0.650152683258\n",
      "Iter 75000, Batch Loss= 0.648106, Accuracy= 0.709333360195\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.676847338676, Accuracy= 0.690193414688\n",
      "Iter 90000, Batch Loss= 0.577636, Accuracy= 0.750666677952\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.658200621605, Accuracy= 0.709195792675\n",
      "Iter 105000, Batch Loss= 0.459725, Accuracy= 0.813333332539\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.564512073994, Accuracy= 0.77740073204\n",
      "Iter 120000, Batch Loss= 0.343829, Accuracy= 0.861333310604\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.510458290577, Accuracy= 0.798439085484\n",
      "Iter 135000, Batch Loss= 0.268613, Accuracy= 0.893999993801\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.506120681763, Accuracy= 0.822192072868\n",
      "Iter 150000, Batch Loss= 0.171295, Accuracy= 0.949333310127\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.400634557009, Accuracy= 0.858839511871\n",
      "Iter 165000, Batch Loss= 0.228940, Accuracy= 0.910000026226\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.528801441193, Accuracy= 0.823210060596\n",
      "Iter 180000, Batch Loss= 0.244825, Accuracy= 0.915333330631\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.42062073946, Accuracy= 0.846284329891\n",
      "Iter 195000, Batch Loss= 0.289663, Accuracy= 0.888666689396\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.370230674744, Accuracy= 0.868340671062\n",
      "Iter 210000, Batch Loss= 0.240849, Accuracy= 0.914666652679\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.438254743814, Accuracy= 0.868340671062\n",
      "Iter 225000, Batch Loss= 0.227465, Accuracy= 0.924000024796\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.378551691771, Accuracy= 0.878859877586\n",
      "Iter 240000, Batch Loss= 0.124946, Accuracy= 0.967999994755\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.373445153236, Accuracy= 0.879199206829\n",
      "Iter 255000, Batch Loss= 0.095721, Accuracy= 0.966666638851\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.366693913937, Accuracy= 0.889039695263\n",
      "Iter 270000, Batch Loss= 0.100488, Accuracy= 0.967999994755\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.380916416645, Accuracy= 0.890057682991\n",
      "Iter 285000, Batch Loss= 0.083806, Accuracy= 0.981999993324\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.387253373861, Accuracy= 0.885646402836\n",
      "Iter 300000, Batch Loss= 0.058355, Accuracy= 0.98400002718\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.362882614136, Accuracy= 0.885985732079\n",
      "Iter 315000, Batch Loss= 0.048403, Accuracy= 0.995333313942\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.360290080309, Accuracy= 0.883949756622\n",
      "Iter 330000, Batch Loss= 0.123106, Accuracy= 0.97000002861\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.378132462502, Accuracy= 0.880895853043\n",
      "Iter 345000, Batch Loss= 0.146247, Accuracy= 0.947333335876\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.378316938877, Accuracy= 0.882592439651\n",
      "Iter 360000, Batch Loss= 0.154971, Accuracy= 0.932666659355\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.365395456553, Accuracy= 0.883949756622\n",
      "Iter 375000, Batch Loss= 0.168126, Accuracy= 0.926666676998\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.332006692886, Accuracy= 0.894129633904\n",
      "Iter 390000, Batch Loss= 0.162079, Accuracy= 0.931999981403\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.33190792799, Accuracy= 0.895486950874\n",
      "Iter 405000, Batch Loss= 0.103533, Accuracy= 0.953999996185\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.377942830324, Accuracy= 0.890057682991\n",
      "Iter 420000, Batch Loss= 0.089143, Accuracy= 0.945999979973\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.358698666096, Accuracy= 0.890397012234\n",
      "Iter 435000, Batch Loss= 0.085800, Accuracy= 0.954666674137\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.378321945667, Accuracy= 0.894129633904\n",
      "Iter 450000, Batch Loss= 0.091998, Accuracy= 0.959333360195\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.354800641537, Accuracy= 0.897183597088\n",
      "Iter 465000, Batch Loss= 0.106887, Accuracy= 0.945333361626\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.380411058664, Accuracy= 0.897183597088\n",
      "Iter 480000, Batch Loss= 0.107305, Accuracy= 0.946666657925\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.397990554571, Accuracy= 0.895486950874\n",
      "Iter 495000, Batch Loss= 0.075448, Accuracy= 0.979333341122\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.41429451108, Accuracy= 0.886325061321\n",
      "Iter 510000, Batch Loss= 0.090638, Accuracy= 0.963999986649\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.312687814236, Accuracy= 0.905327439308\n",
      "Iter 525000, Batch Loss= 0.060556, Accuracy= 0.983333349228\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.360934734344, Accuracy= 0.893111646175\n",
      "Iter 540000, Batch Loss= 0.133916, Accuracy= 0.940666675568\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.37526011467, Accuracy= 0.894468963146\n",
      "Iter 555000, Batch Loss= 0.172782, Accuracy= 0.930000007153\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.378896504641, Accuracy= 0.900237500668\n",
      "Iter 570000, Batch Loss= 0.164935, Accuracy= 0.932666659355\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.409483522177, Accuracy= 0.896504938602\n",
      "Iter 585000, Batch Loss= 0.170681, Accuracy= 0.926666676998\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.395850330591, Accuracy= 0.891075670719\n",
      "Iter 600000, Batch Loss= 0.158934, Accuracy= 0.932666659355\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.327133059502, Accuracy= 0.903970122337\n",
      "Iter 615000, Batch Loss= 0.092210, Accuracy= 0.973333358765\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.333063930273, Accuracy= 0.901255488396\n",
      "Iter 630000, Batch Loss= 0.071602, Accuracy= 0.981333315372\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.360747814178, Accuracy= 0.900916159153\n",
      "Iter 645000, Batch Loss= 0.070426, Accuracy= 0.983333349228\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.368160426617, Accuracy= 0.897862255573\n",
      "Iter 660000, Batch Loss= 0.057676, Accuracy= 0.986000001431\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.353519767523, Accuracy= 0.898540914059\n",
      "Iter 675000, Batch Loss= 0.049986, Accuracy= 0.986666679382\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.353590637445, Accuracy= 0.897862255573\n",
      "Iter 690000, Batch Loss= 0.039755, Accuracy= 0.990000009537\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.376369863749, Accuracy= 0.89616560936\n",
      "Iter 705000, Batch Loss= 0.106671, Accuracy= 0.971333324909\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.373999327421, Accuracy= 0.898540914059\n",
      "Iter 720000, Batch Loss= 0.131716, Accuracy= 0.952000021935\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.413214832544, Accuracy= 0.891754329205\n",
      "Iter 735000, Batch Loss= 0.145185, Accuracy= 0.948000013828\n",
      "test_dir SET DISPLAY STEP: Batch Loss= 0.364172637463, Accuracy= 0.895486950874\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss= 0.364172637463, Accuracy= 0.895486950874\n"
     ]
    }
   ],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" iterations at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs =         extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))\n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys,\n",
    "            istate: np.zeros((batch_size, 2*n_hidden))\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print \"Iter \" + str(step*batch_size) + \\\n",
    "              \", Batch Loss= \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy= \" + \"{}\".format(acc)\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test),\n",
    "                istate: np.zeros((len(X_test), 2*n_hidden))\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print \"FIXED DISPLAY STEP: \" + \\\n",
    "              \"Batch Loss= {}\".format(loss) + \\\n",
    "              \", Accuracy= \" + \"{}\".format(acc)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print \"Optimization Finished!\"\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test),\n",
    "        istate: np.zeros((len(X_test), 2*n_hidden))\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print \"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss= {}\".format(final_loss) + \\\n",
    "      \", Accuracy= \" + \"{}\".format(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training is good, but having visual insight is even better:\n",
    "\n",
    "Okay, let's do it simply in the notebook for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:37:45.172123",
     "start_time": "2016-10-25T13:37:44.691070"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1] + [training_iters])\n",
    "plt.plot(indep_test_axis, np.array(test_losses),     \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, the multi-class confusion matrix and metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:39:56.840147",
     "start_time": "2016-10-25T13:39:56.079496"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "print \"Testing Accuracy: {}%\".format(100*accuracy)\n",
    "\n",
    "print \"\"\n",
    "print \"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\"))\n",
    "print \"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\"))\n",
    "print \"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\"))\n",
    "\n",
    "print \"\"\n",
    "print \"Confusion Matrix:\"\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "print confusion_matrix\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print \"\"\n",
    "print \"Confusion matrix (normalised to % of total test data):\"\n",
    "print normalised_confusion_matrix\n",
    "print (\"Note: training and testing data is not equally distributed amongst classes, \"\n",
    "       \"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, labels, rotation=90)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-25T13:54:36.329206",
     "start_time": "2016-10-25T13:54:36.324081"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Outstandingly, the accuracy is of 89.888%! \n",
    "\n",
    "This means that the neural networks is almost always able to correctly identify the movement type! Remember, the phone is attached on the waist and each series to classify has just a 128 sample window of two internal sensors (a.k.a. 2.56 seconds at 50 FPS), so those predictions are extremely accurate.\n",
    "\n",
    "I specially did not expect such good results for guessing between \"WALKING\" \"WALKING_UPSTAIRS\" and \"WALKING_DOWNSTAIRS\" as a cellphone. Tought, it is still possible to see a little cluster on the matrix between those 3 classes. This is great.\n",
    "\n",
    "It is also possible to see that it was hard to do the difference between \"SITTING\" and \"STANDING\". Those are seemingly almost the same thing from the point of view of a device placed at waist level. \n",
    "\n",
    "## References\n",
    "\n",
    "The [dataset](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) is described on the UCI Machine Learning Repository:\n",
    "\n",
    "> Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
