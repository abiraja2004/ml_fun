{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# basic libs #\n",
    "##############\n",
    "\n",
    "from subprocess import call\n",
    "from tqdm import *\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, sys, time, datetime, json, string, glob, re, random\n",
    "random.seed(2016)\n",
    "\n",
    "###########\n",
    "# science #\n",
    "###########\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#######\n",
    "# ML #\n",
    "######\n",
    "\n",
    "import theano as thno\n",
    "import theano.tensor as T\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn import pipeline\n",
    "from sklearn import feature_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set(style=\"dark\", palette=\"muted\")\n",
    "sns.set_context(\"notebook\",\n",
    "                font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ken Cavagnolo \n",
      "Last updated: Wed Feb 17 2016 \n",
      "\n",
      "CPython 2.7.10\n",
      "IPython 4.0.3\n",
      "\n",
      "numpy 1.10.4\n",
      "scipy 0.17.0\n",
      "pandas 0.17.1\n",
      "seaborn 0.7.0\n",
      "scikit-learn 0.17\n",
      "joblib 0.9.4\n",
      "\n",
      "compiler   : GCC 5.2.1 20151010\n",
      "system     : Linux\n",
      "release    : 4.2.0-23-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : ubuntu\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Ken Cavagnolo\" -n -u -v -m -h -p numpy,scipy,pandas,seaborn,scikit-learn,joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_stemmer(s):\n",
    "    slist = [stemmer.stem(w) for w in s]\n",
    "    s = \" \".join(slist)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_stem(s):\n",
    "    if isinstance(s, basestring):\n",
    "        s = s.lower()\n",
    "        s = s.replace(\"'\",\"in.\")\n",
    "        s = s.replace(\"inches\",\"in.\")\n",
    "        s = s.replace(\"inch\",\"in.\")\n",
    "        s = s.replace(\" in \",\"in. \")\n",
    "        s = s.replace(\" in.\",\"in.\")\n",
    "        s = s.replace(\"''\",\"ft.\")\n",
    "        s = s.replace(\" feet \",\"ft. \")\n",
    "        s = s.replace(\"feet\",\"ft.\")\n",
    "        s = s.replace(\"foot\",\"ft.\")\n",
    "        s = s.replace(\" ft \",\"ft. \")\n",
    "        s = s.replace(\" ft.\",\"ft.\")\n",
    "        s = s.replace(\" pounds \",\"lb. \")\n",
    "        s = s.replace(\" pound \",\"lb. \")\n",
    "        s = s.replace(\"pound\",\"lb.\")\n",
    "        s = s.replace(\" lb \",\"lb. \")\n",
    "        s = s.replace(\" lb.\",\"lb.\")\n",
    "        s = s.replace(\" lbs \",\"lb. \")\n",
    "        s = s.replace(\"lbs.\",\"lb.\")\n",
    "        s = s.replace(\" x \",\" xby \")\n",
    "        s = s.replace(\"*\",\" xby \")\n",
    "        s = s.replace(\" by \",\" xby\")\n",
    "        s = s.replace(\"x0\",\" xby 0\")\n",
    "        s = s.replace(\"x1\",\" xby 1\")\n",
    "        s = s.replace(\"x2\",\" xby 2\")\n",
    "        s = s.replace(\"x3\",\" xby 3\")\n",
    "        s = s.replace(\"x4\",\" xby 4\")\n",
    "        s = s.replace(\"x5\",\" xby 5\")\n",
    "        s = s.replace(\"x6\",\" xby 6\")\n",
    "        s = s.replace(\"x7\",\" xby 7\")\n",
    "        s = s.replace(\"x8\",\" xby 8\")\n",
    "        s = s.replace(\"x9\",\" xby 9\")\n",
    "        s = s.replace(\"0x\",\"0 xby \")\n",
    "        s = s.replace(\"1x\",\"1 xby \")\n",
    "        s = s.replace(\"2x\",\"2 xby \")\n",
    "        s = s.replace(\"3x\",\"3 xby \")\n",
    "        s = s.replace(\"4x\",\"4 xby \")\n",
    "        s = s.replace(\"5x\",\"5 xby \")\n",
    "        s = s.replace(\"6x\",\"6 xby \")\n",
    "        s = s.replace(\"7x\",\"7 xby \")\n",
    "        s = s.replace(\"8x\",\"8 xby \")\n",
    "        s = s.replace(\"9x\",\"9 xby \")\n",
    "        s = s.replace(\" sq ft\",\"sq.ft. \")\n",
    "        s = s.replace(\"sq ft\",\"sq.ft. \")\n",
    "        s = s.replace(\"sqft\",\"sq.ft. \")\n",
    "        s = s.replace(\" sqft \",\"sq.ft. \")\n",
    "        s = s.replace(\"sq. ft\",\"sq.ft. \")\n",
    "        s = s.replace(\"sq ft.\",\"sq.ft. \")\n",
    "        s = s.replace(\"sq feet\",\"sq.ft. \")\n",
    "        s = s.replace(\"square feet\",\"sq.ft. \")\n",
    "        s = s.replace(\" gallons \",\"gal. \")\n",
    "        s = s.replace(\" gallon \",\"gal. \")\n",
    "        s = s.replace(\"gallons\",\"gal.\")\n",
    "        s = s.replace(\"gallon\",\"gal.\")\n",
    "        s = s.replace(\" gal \",\"gal. \")\n",
    "        s = s.replace(\" gal\",\"gal.\")\n",
    "        s = s.replace(\"ounces\",\"oz.\")\n",
    "        s = s.replace(\"ounce\",\"oz.\")\n",
    "        s = s.replace(\" oz.\",\"oz. \")\n",
    "        s = s.replace(\" oz \",\"oz. \")\n",
    "        s = s.replace(\"centimeters\",\"cm.\")\n",
    "        s = s.replace(\" cm.\",\"cm.\")\n",
    "        s = s.replace(\" cm \",\"cm. \")\n",
    "        s = s.replace(\"milimeters\",\"mm.\")\n",
    "        s = s.replace(\" mm.\",\"mm.\")\n",
    "        s = s.replace(\" mm \",\"mm. \")\n",
    "        s = s.replace(u\"\\u00b0\",\"deg. \")\n",
    "        s = s.replace(\"degrees\",\"deg. \")\n",
    "        s = s.replace(\"degree\",\"deg. \")\n",
    "        s = s.replace(\"volts\",\"volt. \")\n",
    "        s = s.replace(\"volt\",\"volt. \")\n",
    "        s = s.replace(\"watts\",\"watt. \")\n",
    "        s = s.replace(\"watt\",\"watt. \")\n",
    "        s = s.replace(\"ampere\",\"amp. \")\n",
    "        s = s.replace(\"amps\",\"amp. \")\n",
    "        s = s.replace(\" amp \",\"amp. \")\n",
    "        s = s.replace(\"whirpool\",\"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolga\", \"whirlpool\")\n",
    "        s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        s = str_stemmer(s.lower().split())\n",
    "        return s.lower()\n",
    "    else:\n",
    "        return \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_common_word(str1, str2):\n",
    "    return sum(int(str2.find(word) >= 0) for word in str1.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = metrics.mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, hd_searches):\n",
    "        d_col_drops=['id','relevance','search_term',\n",
    "                     'product_title','product_description',\n",
    "                     'product_info','attr','brand']\n",
    "        hd_searches = hd_searches.drop(d_col_drops,axis=1).values\n",
    "        return hd_searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = '/Users/cavagnolo/ml_fun/home_depot/data/'\n",
    "\n",
    "df_train = pd.read_csv(datadir + 'train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(datadir + 'test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv(datadir + 'product_descriptions.csv', encoding=\"ISO-8859-1\")\n",
    "df_attr = pd.read_csv(datadir + 'attributes.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')\n",
    "\n",
    "df_all.to_csv('df_all.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "      <th>search_term</th>\n",
       "      <th>product_description</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>100001</td>\n",
       "      <td>3.00</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>100001</td>\n",
       "      <td>2.50</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>100002</td>\n",
       "      <td>3.00</td>\n",
       "      <td>deck over</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "      <td>BEHR Premium Textured DeckOver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.33</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "      <td>Delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>100005</td>\n",
       "      <td>2.67</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "      <td>Delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      product_title  product_uid  \\\n",
       "0   2                  Simpson Strong-Tie 12-Gauge Angle       100001   \n",
       "1   3                  Simpson Strong-Tie 12-Gauge Angle       100001   \n",
       "2   9  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...       100002   \n",
       "3  16  Delta Vero 1-Handle Shower Only Faucet Trim Ki...       100005   \n",
       "4  17  Delta Vero 1-Handle Shower Only Faucet Trim Ki...       100005   \n",
       "\n",
       "   relevance         search_term  \\\n",
       "0       3.00       angle bracket   \n",
       "1       2.50           l bracket   \n",
       "2       3.00           deck over   \n",
       "3       2.33    rain shower head   \n",
       "4       2.67  shower only faucet   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Not only do angles make joints stronger, they ...   \n",
       "1  Not only do angles make joints stronger, they ...   \n",
       "2  BEHR Premium Textured DECKOVER is an innovativ...   \n",
       "3  Update your bathroom with the Delta Vero Singl...   \n",
       "4  Update your bathroom with the Delta Vero Singl...   \n",
       "\n",
       "                            brand  \n",
       "0              Simpson Strong-Tie  \n",
       "1              Simpson Strong-Tie  \n",
       "2  BEHR Premium Textured DeckOver  \n",
       "3                           Delta  \n",
       "4                           Delta  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('data/df_all.csv', encoding=\"ISO-8859-1\")\n",
    "df_all.drop('Unnamed: 0', 1, inplace=True)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['search_term'] = df_all['search_term'].map(lambda x: str_stem(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x: str_stem(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x: str_stem(x))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x: str_stem(x))\n",
    "\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "df_all['len_of_description'] = df_all['product_description'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "\n",
    "df_all['product_info'] = df_all['search_term'] + \"\\t\" + df_all['product_title'] + \"\\t\" + df_all['product_description']\n",
    "df_all['attr'] = df_all['search_term'] + \"\\t\" + df_all['brand']\n",
    "\n",
    "df_all['query_in_title'] = df_all['product_info'].map(lambda x: str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "df_all['query_in_description'] = df_all['product_info'].map(lambda x: str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "\n",
    "df_all['word_in_title'] = df_all['product_info'].map(lambda x: str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['word_in_description'] = df_all['product_info'].map(lambda x: str_common_word(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "\n",
    "df_all['ratio_title'] = df_all['word_in_title'] / df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['word_in_description'] / df_all['len_of_query']\n",
    "df_all['ratio_brand'] = df_all['word_in_brand'] / df_all['len_of_brand']\n",
    "\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=1\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x: d[x])\n",
    "df_all['search_term_feature'] = df_all['search_term'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['is_train'] = False\n",
    "num_train = df_train.shape[0]\n",
    "df_all['is_train'].iloc[:num_train] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to hdf5 for easier loading later\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_test', df_test)\n",
    "hdf.put('df_train', df_train)\n",
    "hdf.put('df_all', df_all)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/kcavagnolo/ml_fun/home_depot/data/features.h5\n",
      "/df_all              frame        (shape->[240760,3])\n",
      "/df_test             frame        (shape->[166693,3])\n",
      "/df_train            frame        (shape->[74067,3]) \n"
     ]
    }
   ],
   "source": [
    "# get datadir\n",
    "import platform\n",
    "uname = platform.uname()[0]\n",
    "if uname == 'Linux':\n",
    "    datadir = '/home/kcavagnolo/ml_fun/home_depot/data/'\n",
    "elif uname == 'Darwin':\n",
    "    datadir = '/Users/cavagnolo/ml_fun/home_depot/data/'\n",
    "else:\n",
    "    raise OSError(\"Unknown system: \"+str(uname))\n",
    "\n",
    "# get files\n",
    "files = sorted(glob.glob(datadir + '*.csv'))\n",
    "hdf_file = datadir + 'features.h5'\n",
    "\n",
    "# reopen hdf store\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "\n",
    "# reload df's\n",
    "print hdf\n",
    "df_all = hdf['df_all']\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_all[(df_all.is_train == True)].copy().drop('is_train', 1)\n",
    "df_test = df_all[(df_all.is_train == False)].copy().drop('is_train', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in data\n",
    "\n",
    "id_test  = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "X_train = df_train[:]\n",
    "X_test  = df_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf = RandomForestRegressor(n_estimators = 1000,\n",
    "                           random_state = 2016,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 1)\n",
    "\n",
    "# \n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                        stop_words='english')\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=12,\n",
    "                    random_state = 2016)\n",
    "\n",
    "combined_features = pipeline.FeatureUnion([('cst',  cust_regression_vals()),\n",
    "                                  ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='search_term')),\n",
    "                                                              ('tfidf1', tfidf),\n",
    "                                                              ('tsvd1', tsvd)])),\n",
    "                                  ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='product_title')),\n",
    "                                                              ('tfidf2', tfidf),\n",
    "                                                              ('tsvd2', tsvd)])),\n",
    "                                  ('txt3', pipeline.Pipeline([('s3', cust_txt_col(key='product_description')),\n",
    "                                                              ('tfidf3', tfidf),\n",
    "                                                              ('tsvd3', tsvd)])),\n",
    "                                  ('txt4', pipeline.Pipeline([('s4', cust_txt_col(key='brand')),\n",
    "                                                              ('tfidf4', tfidf),\n",
    "                                                              ('tsvd4', tsvd)]))\n",
    "                                 ],\n",
    "                                 n_jobs = 1,\n",
    "                                 transformer_weights= {'cst': 1.0,\n",
    "                                                       'txt1': 0.5,\n",
    "                                                       'txt2': 0.25,\n",
    "                                                       'txt3': 0.0,\n",
    "                                                       'txt4': 0.5},\n",
    "                                )\n",
    "\n",
    "clf = pipeline.Pipeline([('features', combined_features), ('rf', rf)])\n",
    "\n",
    "param_grid = {'rf__max_features': [12],\n",
    "              'rf__max_depth': [20],\n",
    "              'features__txt3__tsvd3__n_components': [6, 10, 12, 16]\n",
    "             }\n",
    "\n",
    "RMSE = metrics.make_scorer(fmean_squared_error, greater_is_better=False)\n",
    "\n",
    "grid_model = grid_search.GridSearchCV(estimator = clf,\n",
    "                                      param_grid = param_grid,\n",
    "                                      cv = 5,\n",
    "                                      scoring = RMSE,\n",
    "                                      n_jobs = -1,\n",
    "                                      verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "[joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-03f1c7e102ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         \"\"\"\n\u001b[1;32m--> 804\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 for train, test in cv)\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_pool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_initialize_pool\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0malready_forked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0malready_forked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m                     raise ImportError('[joblib] Attempting to do parallel computing '\n\u001b[0m\u001b[0;32m    519\u001b[0m                             \u001b[1;34m'without protecting your import on a system that does '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                             \u001b[1;34m'not support forking. To use parallel-computing in a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: [joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information"
     ]
    }
   ],
   "source": [
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best score: -0.47121\n",
      "\n",
      "Best parameters set:\n",
      "\trf__max_depth: 20\n",
      "\trf__max_features: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest score: {:0.5f}\".format(grid_model.best_score_))\n",
    "print(\"\\nBest parameters set:\")\n",
    "best_parameters = grid_model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t{}: {}\".format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the below use:\n",
    "n_comps = 12,\n",
    "cst: 1.0,\n",
    "txt1: 0.5,\n",
    "txt2: 0.25,\n",
    "txt3: 0.0,\n",
    "txt4: 0.5\n",
    "\n",
    "N_comps | Max Feature | Max Depth | Local   | KLB\n",
    ":--     | :--         | :--       | :--:    | :--:\n",
    "10*     | 10*, 20     | 10*, 20   | 0.47551 | 0.47415\n",
    "12*     | 12*         | 20*       | 0.47133 | 0.47373\n",
    "12*     | 12*, 16, 18 | None*     | 0.47102 | 0.47423\n",
    "\n",
    "All the below use:\n",
    "cst: 1.0,\n",
    "txt1: 0.5,\n",
    "txt2: 0.15,\n",
    "txt3: 0.1,\n",
    "txt4: 0.5\n",
    "\n",
    "N_comps | Max Feature | Max Depth | Local   | KLB\n",
    ":--     | :--         | :--       | :--:    | :--:\n",
    "12*     | 12*         | 20*       | 0.47121 | 0.47423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "pd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to hdf5 for easier loading later\n",
    "call([\"rm\", \"-rf\", hdf_file])\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_test', df_test)\n",
    "hdf.put('df_train', df_train)\n",
    "hdf.put('df_all', df_all)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
