{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# basic libs #\n",
    "##############\n",
    "\n",
    "from subprocess import call\n",
    "from tqdm import *\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, sys, time, datetime, json, string, glob, re\n",
    "\n",
    "############\n",
    "# plumbing #\n",
    "############\n",
    "\n",
    "import tempfile\n",
    "from joblib import Memory\n",
    "cachedir = tempfile.mkdtemp()\n",
    "mem = Memory(cachedir=cachedir, verbose=1)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from collections import OrderedDict\n",
    "import urllib2, requests, operator, hashlib, uuid, itertools\n",
    "\n",
    "###########\n",
    "# science #\n",
    "###########\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#######\n",
    "# ML #\n",
    "######\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano as thno\n",
    "import theano.tensor as T\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import grid_search\n",
    "from sklearn import pipeline\n",
    "from sklearn import feature_selection\n",
    "\n",
    "#################\n",
    "# visualization #\n",
    "#################\n",
    "\n",
    "# plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='pearl')\n",
    "tls.set_credentials_file(username=os.environ.get('PLOTLY_USERNAME'), api_key=os.environ.get('PLOTLY_APIKEY'))\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set(style=\"dark\", palette=\"muted\")\n",
    "sns.set_context(\"notebook\",\n",
    "                font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "#graphistry\n",
    "import graphistry\n",
    "graphistry.register(key=os.environ.get('GRAPHISTRY_APIKEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ken Cavagnolo \n",
      "Last updated: Tue Feb 16 2016 \n",
      "\n",
      "CPython 2.7.10\n",
      "IPython 4.0.3\n",
      "\n",
      "numpy 1.10.4\n",
      "scipy 0.17.0\n",
      "pandas 0.17.1\n",
      "seaborn 0.7.0\n",
      "scikit-learn 0.17\n",
      "\n",
      "compiler   : GCC 5.2.1 20151010\n",
      "system     : Linux\n",
      "release    : 4.2.0-23-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : ubuntu\n",
      "Git hash   : a2c2b27acf49b258e1702aa8ce17d34fbb0bd4d0\n"
     ]
    }
   ],
   "source": [
    "%watermark -a \"Ken Cavagnolo\" -n -u -v -m -h -g -p numpy,scipy,pandas,seaborn,scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kscore(y_pred, y_true):\n",
    "    nrows = y_pred.shape[0]\n",
    "    y_pred = y_pred + 1e-15\n",
    "    probs = y_pred[list(range(nrows)), y_true.astype(int)]\n",
    "    return - np.sum(np.log(probs)) / nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def factors(n):    \n",
    "    return set(reduce(list.__add__,\n",
    "                      ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_indices(df, attr):\n",
    "    idx = {}\n",
    "    for a in attr:\n",
    "        idx[a] = [c for c in df.columns if c.startswith(a)]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_combos(attr):\n",
    "    combos = []\n",
    "    for i in range(1, len(attr)+1):\n",
    "        for subset in itertools.combinations(attr, i):\n",
    "            combos.append(subset)\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@mem.cache\n",
    "def run_cv(df, attr, clf_class, **kwargs):\n",
    "    \n",
    "    # scoring arrays\n",
    "    acc = []\n",
    "    prec = []\n",
    "    recl = []\n",
    "    f1 = []\n",
    "    \n",
    "    # get attribute combos to explore\n",
    "    attr_com = all_combos(attr)\n",
    "\n",
    "    # get these attr as indices\n",
    "    attr_idx = feature_indices(df, attr)\n",
    "    \n",
    "    # iterate through all combinations of features\n",
    "    for a in attr_com:\n",
    "        idxs = []\n",
    "        for b in a:\n",
    "            idxs.extend(attr_idx[b])\n",
    "        X = df.ix[:, (idxs)].copy().values\n",
    "        y = df.fault_severity.values\n",
    "    \n",
    "        # run kfold cv\n",
    "        kf = cross_validation.KFold(len(y), n_folds=10, shuffle=True)\n",
    "        y_pred = y.copy()\n",
    "        mean_acc = 0.0\n",
    "        mean_prec = 0.0\n",
    "        mean_recl = 0.0\n",
    "        mean_f1 = 0.0\n",
    "        for train_index, test_index in kf:\n",
    "            clf = clf_class(**kwargs)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred[test_index] = clf.predict(X_test)\n",
    "            mean_acc += metrics.accuracy_score(y[test_index], y_pred[test_index])\n",
    "            mean_recl += metrics.recall_score(y[test_index], y_pred[test_index])\n",
    "            mean_prec += metrics.precision_score(y[test_index], y_pred[test_index])\n",
    "            mean_f1 += metrics.f1_score(y[test_index], y_pred[test_index])\n",
    "        acc.append(mean_acc / len(kf))\n",
    "        recl.append(mean_recl / len(kf))\n",
    "        prec.append(mean_prec / len(kf))\n",
    "        f1.append(mean_f1 / len(kf))\n",
    "    result = pd.DataFrame({'attr_combo':attr_com, 'accuracy':acc, 'precision':prec, 'recall':recl, 'f1':f1})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What's the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Informal: using Telstra's service logs, the task is to predict the severity of a network disruption.\n",
    "\n",
    "* Describe the problem formally, e.g.\n",
    "    * Task (T): For a specific time and location, classify network fault severity as 0 == no faults, 1 == few faults, 2 == many faults. Fault severity is a measurement of actual reported faults from users of the network and is the target variable.\n",
    "    * Experience (E): Each row in the train/test data represents a location and a time point. They are identified by the \"id\" column, which is the key \"id\" used in the other data files (event_type, resource_type, severity_type, log_feature) that contain features extracted from log files and other sources. Note that severity_type:\n",
    "        * Is a categorical feature extracted from the log files\n",
    "        * \"Often\" (**when is it not?**) the type of the warning message coming from the log files\n",
    "        * Does not have an ordering\n",
    "    * Performance (P): Classification accuracy, the number of fault events predicted correctly (a set of predicted probabilities one for every fault severity mus be given) out of all faults considered as a percentage.\n",
    "\n",
    "* Assumptions:\n",
    "    * All given features will matter to the model.\n",
    "    * The clustering in time and location matter to the model, but may be mutually exclusive.\n",
    "    * The volume of faults may not matter to the model.\n",
    "    * Systemic long duration, low-volume correlated faults could be hiding in data.\n",
    "\n",
    "* Similar problems:\n",
    "    * Airline loyalty and medallion class problem\n",
    "    * Anomaly detection, e.g. [Twitter's R package](https://github.com/nicolasmiller/pyculiarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# How to approach the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Brainstorm! Literally type everything as a stream of consciousness (coffee helps here)\n",
    "\n",
    "These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts. These are my thoughts.\n",
    "\n",
    "* Describe how to solve problem manually\n",
    "* What data do I have?\n",
    "* What data do I need?\n",
    "* What data I **don't** need?\n",
    "* What do I know the least about in the solution process above?\n",
    "    * knowledge gap 1\n",
    "    * knowledge gap 2\n",
    "* What am I concerned is incorrect in above solution? Find an expert online and ask them about these items\n",
    "    * concern 1\n",
    "    * concern 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# go through each file and tell me what's in it\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    cols = df.columns.tolist()\n",
    "    obs = df.shape[0]\n",
    "    print '{:15} {:s}'.format('Filename:', f)\n",
    "    print '{:15} {:d}'.format('Observations:', obs)\n",
    "    for c in cols:\n",
    "        uflag = \"\"\n",
    "        nflag = \"\"\n",
    "        uni = len(pd.unique(df[c].ravel()))\n",
    "        nulls = np.count_nonzero(df[c].isnull())\n",
    "        if obs != uni:\n",
    "            uflag = '*MULTIPLE VALS PER KEY*'\n",
    "        if nulls >0 :\n",
    "            nflag = '*NULLS IN COL*'\n",
    "        print '{:15} {:d} {:20} {:20}'.format(c, uni, uflag, nflag)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Column names look fine, don't need to clean those\n",
    "* No nulls, hooray\n",
    "* Looks like each ID has only one severity type and multiple event types| log features| resource types:\n",
    "\n",
    "For example, in the training set, consider ID 10005:\n",
    "\n",
    "|id| item | code | sev/vol\n",
    "|:-|:-|:-:|:-:\n",
    "|10005 | location |802 | 0\n",
    "|10005|severity_type |1|\n",
    "|10005|event_type |11|\n",
    "|10005|event_type |13|\n",
    "|10005|event_type |14|\n",
    "|10005|log_feature| 345|1\n",
    "|10005|log_feature| 54|1\n",
    "|10005|resource_type |4|\n",
    "|10005|resource_type |6|\n",
    "|10005|resource_type |8|\n",
    "\n",
    "Transposing would be\n",
    "\n",
    "|id|loc|sev|evt|fea|res\n",
    "|-|\n",
    "|10005|{802:0}|1|[11,13,14]|{54:1,345:1}|[4,6,8]\n",
    "\n",
    "For a single id, the coding can't be a list, so one hot encode every type? How many are we talking?\n",
    "* 53 event types (binary)\n",
    "* 386 log features (continuous)\n",
    "* 10 resource types (binary)\n",
    "\n",
    "That's 449 features. Regardless, need to join all this data together. It's all categorical data (code_n is y/n), except for the log features which has a volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replace missing values\n",
    "* Remove duplicates\n",
    "* One-Hot encode categorical features\n",
    "* Find outliers and explain\n",
    "* Scale\n",
    "* Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read raw volumes\n",
    "df = pd.read_csv(files[1])\n",
    "\n",
    "# cleanup col names\n",
    "for c in df.columns:\n",
    "    if df[c].dtypes == 'object':\n",
    "        df[c] = df[c].map(lambda x: re.sub(\"[^0-9]\", \"\", x))\n",
    "        \n",
    "# re-org by id        \n",
    "vols = df.pivot(index='id', columns='log_feature', values='volume').fillna(0)\n",
    "cnames = ['log_feature_'+str(x) for x in vols.columns]\n",
    "\n",
    "# scale or min-max\n",
    "vols_scl = vols.apply(lambda x: preprocessing.MinMaxScaler().fit_transform(x))\n",
    "vols_scl.columns = cnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build train df\n",
    "df = pd.read_csv(files[5], index_col=0)\n",
    "for c in df.columns:\n",
    "    if df[c].dtypes == 'object':\n",
    "        df[c] = df[c].map(lambda x: re.sub(\"[^0-9]\", \"\", x))\n",
    "df_train = df.copy()\n",
    "\n",
    "# build test df\n",
    "df = pd.read_csv(files[4], index_col=0)\n",
    "for c in df.columns:\n",
    "    if df[c].dtypes == 'object':\n",
    "        df[c] = df[c].map(lambda x: re.sub(\"[^0-9]\", \"\", x))\n",
    "df_test = df.copy()\n",
    "\n",
    "print df_train.shape\n",
    "print df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add categorical event_type, resource_type, severity_type\n",
    "nfiles = [files[0], files[2], files[3]]\n",
    "for f in nfiles:\n",
    "    df = pd.read_csv(f, index_col=0)\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].map(lambda x: re.sub(\"[^0-9]\", \"\", x))\n",
    "    dfv = pd.get_dummies(df).groupby(df.index).sum()\n",
    "    df_train = df_train.join(dfv)\n",
    "    df_test = df_test.join(dfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join scaled volumes\n",
    "df_train = df_train.join(vols_scl)\n",
    "df_test = df_test.join(vols_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gut check the 10005 entry from above\n",
    "junk = df_train.T\n",
    "x = junk.loc[:, 10005] > 0\n",
    "x[x].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one time analysis to re-bin volunme data\n",
    "#df = pd.read_csv(files[1], index_col=0)\n",
    "#x = np.log10(df['volume'][(df.volume > 0)].values)\n",
    "#sns.distplot(x, bins=8, kde=False, fit=stats.expon)\n",
    "\n",
    "# grab the bins in log space use the built-in Freedman-Diaconis rule\n",
    "#hist, bins = np.histogram(x, bins=8)\n",
    "\n",
    "# assign each volume to a bin\n",
    "#inds = np.digitize(x, bins)\n",
    "\n",
    "# add this to the df\n",
    "#df['volume'] = inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge df's\n",
    "df_train.reset_index(inplace=True)\n",
    "df_test.reset_index(inplace=True)\n",
    "df_train['is_train'] = True\n",
    "df_test['is_train'] = False\n",
    "df_all = pd.concat([df_train, df_test], axis=0, ignore_index=False)\n",
    "a = df_train.shape[0]\n",
    "b = df_test.shape[0]\n",
    "c = df_all.shape[0]\n",
    "assert(a + b == c, \"DF's are not summing correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 for easier loading later\n",
    "call([\"rm\", \"-rf\", hdf_file])\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_test', df_test)\n",
    "hdf.put('df_train', df_train)\n",
    "hdf.put('df_all', df_all)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reload Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: /home/kcavagnolo/ml_fun/telstra/data/features.h5\n",
      "/df_all              frame        (shape->[18552,458])\n",
      "/df_test             frame        (shape->[11171,457])\n",
      "/df_train            frame        (shape->[7381,458]) \n"
     ]
    }
   ],
   "source": [
    "# get datadir\n",
    "import platform\n",
    "uname = platform.uname()[0]\n",
    "if uname == 'Linux':\n",
    "    datadir = '/home/kcavagnolo/ml_fun/telstra/data/'\n",
    "elif uname == 'Darwin':\n",
    "    datadir = '/Users/cavagnolo/ml_fun/telstra/data/'\n",
    "else:\n",
    "    raise OSError(\"Unknown system: \"+str(uname))\n",
    "\n",
    "# get files\n",
    "files = sorted(glob.glob(datadir+'*.csv'))\n",
    "hdf_file = datadir + 'features.h5'\n",
    "\n",
    "# reopen hdf store\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "\n",
    "# reload df's\n",
    "print hdf\n",
    "df_test = hdf['df_test']\n",
    "df_train = hdf['df_train']\n",
    "df_all = hdf['df_all']\n",
    "#df_svd = hdf['df_svd']\n",
    "#df_pca = hdf['df_pca']\n",
    "#df_kpca = hdf['df_kpca']\n",
    "#df_lle = hdf['df_lle']\n",
    "#df_tsne = hdf['df_tsne']\n",
    "hdf.close()\n",
    "\n",
    "# relevant columns\n",
    "#svdcols = [c for c in df_svd.columns if c[:3] == 'svd']\n",
    "#pcacols = [c for c in df_pca.columns if c[:3] == 'pca']\n",
    "#kpcacols = [c for c in df_kpca.columns if c[:4] == 'kpca']\n",
    "#llecols = [c for c in df_lle.columns if c[:3] == 'lle']\n",
    "\n",
    "# cols to drop\n",
    "attribute_drop = ['fault_severity', 'id', 'location', 'is_train']\n",
    "\n",
    "# col prefixes\n",
    "attributes = ['event_', 'resource_', 'severity_', 'log_feature_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Attribute Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.filter(regex='log_feature_').mean().order(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['log_feature_312'][(df_all.log_feature_312 > 0)].count().astype(float)/len(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# items to plot\n",
    "a = ['event_', 'resource_', 'severity_', 'log_fea']\n",
    "\n",
    "# build axes\n",
    "mpl_fig = plt.figure()\n",
    "ax1 = mpl_fig.add_subplot(411)\n",
    "ax2 = mpl_fig.add_subplot(412)\n",
    "ax3 = mpl_fig.add_subplot(413)\n",
    "ax4 = mpl_fig.add_subplot(414)\n",
    "axs = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "# iterate over each\n",
    "for cond in [True, False]:\n",
    "    for i, b in enumerate(a):\n",
    "        y = df_all[(df_all.is_train == cond)].filter(regex=b).apply(pd.value_counts).fillna(0).T[0]\n",
    "        y = 100.*(1.0-(y/df_all[(df_all.is_train == cond)].shape[0]))\n",
    "        x = range(1, len(y)+1)\n",
    "        axs[i].bar(x, y, label=a[i])\n",
    "        axs[i].set_ylabel(a[i])\n",
    "py.iplot_mpl(mpl_fig, strip_style=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions looks similar, so train will cv w/ test. Any structure in sequence of ID's?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fault = df_all[(df_all.is_train == True)].fault_severity.astype(float)\n",
    "n = len(fault)\n",
    "plt.bar(np.arange(n), fault)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = df_train.location.astype(float)\n",
    "plt.bar(np.arange(n), loc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both look like noise to me, gonna leave it for now. What about correlations among the various codes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = ['severity_type', 'event_type', 'resource_type']#, 'log_feature']\n",
    "for b in a:\n",
    "\n",
    "    # correlation matrix\n",
    "    c = df_all.filter(regex=b).copy()\n",
    "    corr = c.corr(min_periods=len(c.columns)/10)\n",
    "\n",
    "    # colormap\n",
    "    cmap = sns.light_palette(\"navy\", as_cmap=True, reverse=True)\n",
    "    \n",
    "    # clustered heatmap of distances w/o mask\n",
    "    # dist 0 --> close, 1 --> distant\n",
    "    dsim = 1.0-np.abs(corr)\n",
    "    \n",
    "    # find degenerate params\n",
    "    mask = np.ones(dsim.shape,dtype='bool')\n",
    "    mask[np.triu_indices(len(dsim))] = False\n",
    "    x = ((dsim < 0.2) & mask).values.nonzero()\n",
    "    a = zip(x[0], x[1])\n",
    "    for x, y in a:\n",
    "        print dsim.index[x], dsim.columns[y]\n",
    "\n",
    "    # plot matrix\n",
    "    if len(dsim) > 10:\n",
    "        annot=False\n",
    "    else:\n",
    "        annot=True\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    g = sns.clustermap(data=dsim,\n",
    "                       method='complete',\n",
    "                       metric='correlation',\n",
    "                       cmap=cmap, linewidths=0.5, vmin=0.0, vmax = 1.0,\n",
    "                       annot=annot, fmt='.2f', annot_kws={'size':'10'})\n",
    "    plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log_feature specific\n",
    "c = df_all.filter(regex='log_feature').copy()\n",
    "log_corr = c.corr(min_periods=len(c.columns)/10)\n",
    "log_dsim = 1.0-np.abs(log_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmap = sns.light_palette(\"navy\", as_cmap=True, reverse=True)\n",
    "g = sns.clustermap(data=log_dsim,\n",
    "                   method='complete',\n",
    "                   metric='correlation',\n",
    "                   cmap=cmap,\n",
    "                   xticklabels=False, yticklabels=False,\n",
    "                   linewidths=0.0,\n",
    "                   vmin=0.0,\n",
    "                   vmax = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The serverity types are mostly anti-correlated except type 1 and 2.\n",
    "\n",
    "There is interesting structure in the resource and event types. Makes me more curious about how these events are networked, i.e. communicating to each other? Build out as network problem? Can't: id and loc are 1:1 so the only connections will be among events that are the same, in that they have the same coding across all types. I don't see that as informative to a model.\n",
    "\n",
    "Log features are also clustered.\n",
    "\n",
    "**But, this is clearly a well-defined classification problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "X_train = df_all[(df_all.is_train==True)].copy().drop(drop, 1)\n",
    "xcols   = X_train.columns\n",
    "X_train = X_train.values\n",
    "y_train = df_all[(df_all.is_train==True)].fault_severity.values\n",
    "\n",
    "# random test, should be close to 50%\n",
    "pred_rand = np.random.choice(y_train, len(y_train))\n",
    "print 'Random', metrics.accuracy_score(y_train, pred_rand)\n",
    "\n",
    "# couple models\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import BaggingClassifier as BC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "# classifiers\n",
    "clfs = []\n",
    "clfs.append(('DecTree', DecisionTreeClassifier(max_depth=30)))\n",
    "clfs.append(('Bagging', BaggingClassifier(n_estimators=100, n_jobs=-1)))\n",
    "clfs.append(('RandFor', RandomForestClassifier(n_estimators=3000, n_jobs=-1)))\n",
    "\n",
    "# score each on cv\n",
    "for clf in clfs:\n",
    "    scores = cross_validation.cross_val_score(clf[1], X_train, y_train, cv=5)\n",
    "    print clf[0], scores.mean(), '+/-', scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = run_cv(df_train, attributes, DT, **kwargs)\n",
    "plt.figure(figsize=(16, 6))\n",
    "ax = plt.subplot(111)\n",
    "results.loc[:, 'accuracy'].plot(kind='line', ax=ax, rot=90, marker='o')\n",
    "results.loc[:, 'precision'].plot(kind='line', ax=ax, color='r', rot=90, marker='o')\n",
    "#results.loc[:, 'recall'].plot(kind='line', ax=ax, color='y', rot=90, marker='o')\n",
    "results.loc[:, 'f1'].plot(kind='line', ax=ax, color='g', rot=90, marker='o')\n",
    "plt.xticks(range(len(results.index)), results.attr_combo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "clf = clfs[2]\n",
    "clf = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()), ('clf', clf)])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_train)\n",
    "kscore(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = df_all[(df_all.is_train==False)].copy().drop(drop, 1).values\n",
    "ids = df_all[(df_all.is_train==False)].id.values\n",
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'predict_0': prob[:, 0],\n",
    "        'predict_1': prob[:, 1],\n",
    "        'predict_2': prob[:, 2]})\n",
    "df_result.to_csv('first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data has complex multi-dimensional structure that ML algos know how to find and exploit to make decisions\n",
    "* What's not exposed to the models?\n",
    "    * Events are too dense or too complex for the algo to find. Fix == dim reduction (PCA or SVD)\n",
    "    * How the various events are communicating. Fix == clustering\n",
    "    * There is more attribute aggregation/creation needed. Fix == hyperattributes\n",
    "    * Time and location dependance of the events. Fix == ???\n",
    "\n",
    "* Remove correlated features\n",
    "* Remove features using statistical tests\n",
    "* Try pair-wise feature interactions, e.g. a*b, a-b, a+b, a/b\n",
    "* Try feature transformations, e.g. sqrt(a), log(a), abs(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "X_train = df_all[(df_all.is_train==True)].copy().drop(drop, 1)\n",
    "xcols   = X_train.columns\n",
    "X_train = X_train.values\n",
    "y_train = df_all[(df_all.is_train==True)].fault_severity.values\n",
    "\n",
    "# simple rand forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=3000, n_jobs=-1)\n",
    "scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=15, n_jobs=-1)\n",
    "print scores.mean(), '+/-', scores.std()\n",
    "\n",
    "# top ten features\n",
    "clf = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()), ('clf', clf)])\n",
    "clf.fit(X_train, y_train)\n",
    "fea_impt = zip(xcols, (clf.feature_importances_ * 100.0).astype(int))\n",
    "sorted(fea_impt, key=lambda x: -x[1])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "X = df_train.copy().drop(drop, 1)\n",
    "cols = X.columns\n",
    "X = X.values\n",
    "y = df_train.fault_severity.values.astype(int)\n",
    "\n",
    "mpl_fig = plt.figure(1, figsize=(11,8))\n",
    "plt.clf()\n",
    "\n",
    "X_indices = np.arange(X.shape[-1])\n",
    "\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "selector = SelectPercentile(f_classif, percentile=50)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# get scores\n",
    "scores = -np.log10(np.array(selector.pvalues_))\n",
    "scores[np.isnan(scores)] = 0\n",
    "scores /= scores.max()\n",
    "plt.bar(X_indices - .45, scores,\n",
    "        width=.2, label=r'Univariate score ($-Log(p_{value})$)', color='g')\n",
    "\n",
    "# Compare to the weights of an SVM\n",
    "clf = LinearSVC(C=0.3, penalty=\"l1\", dual=False)\n",
    "clf.fit(X, y)\n",
    "\n",
    "svm_weights = (clf.coef_ ** 2).sum(axis=0)\n",
    "svm_weights /= svm_weights.max()\n",
    "\n",
    "plt.bar(X_indices - .25, svm_weights,\n",
    "        width=.2, label='SVM weight', color='r')\n",
    "\n",
    "clf_selected = LinearSVC(C=0.3, penalty=\"l1\", dual=False)\n",
    "clf_selected.fit(selector.transform(X), y)\n",
    "\n",
    "svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\n",
    "svm_weights_selected /= svm_weights_selected.max()\n",
    "\n",
    "plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n",
    "        width=.2, label='SVM weights after selection', color='b')\n",
    "\n",
    "\n",
    "plt.xlabel('Supported feature number')\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print selector.transform(X).shape[1]\n",
    "uni_fea = Xcols[selector.get_support(indices=True)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get index of max value\n",
    "idx = X_indices[selector.get_support()][svm_weights_selected.argmax()]\n",
    "\n",
    "# get column name\n",
    "Xcols[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well no shit, it's log_feature_203 again. Check the baseline models section, it showed up in the decision tree feature importance list as #1 and being almost 2x more important than #2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to know what features may be the most important before any transformation. Try recursive feature elimination with cross-validation (RFECV). For multiclass, stratified K-fold used by default and shuffle is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "# get training data\n",
    "X = df_all[(df_all.is_train == True)].copy()\n",
    "y = X.fault_severity\n",
    "X.drop(drop, 1, inplace=True)\n",
    "\n",
    "# make df to store results\n",
    "df_frank = pd.DataFrame({'feature':X.columns.values})\n",
    "\n",
    "# create base classifier\n",
    "models =[]\n",
    "models.append(LR(n_jobs=-1))\n",
    "models.append(RF(n_jobs=-1))\n",
    "models.append(SGD(n_jobs=-1))\n",
    "models.append(MNB())\n",
    "\n",
    "# iterate over models\n",
    "for m in models:\n",
    "    mname = str(m).split('(')[0]\n",
    "    rfecv = RFECV(m, cv=3, scoring='accuracy')\n",
    "    rfecv = rfecv.fit(X, y)\n",
    "    print(\"%s Optimal # features: %d\" % (mname, rfecv.n_features_))\n",
    "    c = 'rfecv_'+str(mname[:3])\n",
    "    df_frank[c] = rfecv.ranking_\n",
    "    \n",
    "    # n_features vs cv score\n",
    "    plt.figure()\n",
    "    plt.title(mname)\n",
    "    plt.xlabel(\"# features\")\n",
    "    plt.ylabel(\"CV score\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.axvline(rfecv.n_features_, color='r', linestyle='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB is not a good model, nor SGD. Try SVC linear in their place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "m = SVC(kernel='linear')\n",
    "mname = str(m).split('(')[0]\n",
    "rfecv = RFECV(m, cv=3, scoring='accuracy')\n",
    "rfecv = rfecv.fit(X, y)\n",
    "print(\"%s Optimal # features: %d\" % (mname, rfecv.n_features_))\n",
    "c = 'rfecv_'+str(mname[:3])\n",
    "df_frank[c] = rfecv.ranking_\n",
    "    \n",
    "# n_features vs cv score\n",
    "plt.figure()\n",
    "plt.title(mname)\n",
    "plt.xlabel(\"# features\")\n",
    "plt.ylabel(\"CV score\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.axvline(rfecv.n_features_, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get avg rank and abs rank\n",
    "df_frank['avg_rank'] = df_frank[['rfecv_Log', 'rfecv_Ran', 'rfecv_SVC']].mean(axis=1)\n",
    "df_frank['abs_rank'] = df_frank.avg_rank.rank(method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_frank', df_frank)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "X = df_train.copy().drop(drop, 1)\n",
    "cols = X.columns\n",
    "y = df_train.fault_severity\n",
    "clf = LinearSVC(C=0.3, penalty=\"l1\", dual=False)\n",
    "selector = feature_selection.SelectFromModel(clf, threshold=0.50)\n",
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print selector.transform(X).shape[1]\n",
    "imp_fea = cols[selector.get_support(indices=True)].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matricies show there are redundant values. How many components in the full dataset to preserve? The eigenvalues in SVD should help determine what attributes are valuable. Data is sparse matrix of binary and multiclass values, options for analysis:\n",
    "* [Single value decomposition and Principal component analysis](http://blog.applied.ai/visualising-high-dimensional-data/)\n",
    "* [Kernel PCA](http://sebastianraschka.com/Articles/2014_kernel_pca.html)\n",
    "* [Locally Linear Embedding](http://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html)\n",
    "* [t-Distributed Stochastic Neighbor Embedding](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of the cols that I suspect are not important\n",
    "X = df_all.copy().drop(drop, 1)\n",
    "\n",
    "# orig num comps\n",
    "ncomps = X.shape[1] - 1\n",
    "\n",
    "# run svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=ncomps)\n",
    "svd_fit = svd.fit(X)\n",
    "\n",
    "# print out variance ranges\n",
    "for i in range(0, X.shape[1], 20):\n",
    "    svar = svd_fit.explained_variance_ratio_.cumsum()[i]\n",
    "    if svar >= 0.95:\n",
    "        print('Variance preserved at {:} components == {:.1%}'.format(i, svar))\n",
    "\n",
    "# plot result        \n",
    "ax = pd.Series(svd_fit.explained_variance_ratio_.cumsum()).plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 60 components 98% of variance is preserved, so try keeping just those in a new df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform X\n",
    "ncomps = 60\n",
    "X_svd = TruncatedSVD(n_components=ncomps).fit_transform(X)\n",
    "\n",
    "# save svd to df and add back dropped cols\n",
    "df_svd = pd.DataFrame(X_svd, columns=['svd{}'.format(c) for c in range(ncomps)], index=df_all.index)\n",
    "for c in drop:\n",
    "    df_svd[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "svdcols = [c for c in df_svd.columns if c[:3] == 'svd']\n",
    "\n",
    "# check size\n",
    "assert(df_svd.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_svd', df_svd)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Linear PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get rid of the cols that I suspect are not important\n",
    "X = df_all.copy().drop(drop, 1)\n",
    "\n",
    "# orig num comps\n",
    "ncomps = X.shape[1] - 1\n",
    "\n",
    "# run pca\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "pca = PCA(n_components=ncomps, whiten=True)\n",
    "pca_fit = pca.fit(X)\n",
    "\n",
    "# plot result        \n",
    "ax = pd.Series(pca_fit.explained_variance_ratio_.cumsum()).plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform X\n",
    "ncomps = 2\n",
    "X_pca = PCA(n_components=ncomps, whiten=True).fit_transform(X)\n",
    "\n",
    "# check transform\n",
    "print \"Mean: \", np.round(X_pca.mean(axis=0), decimals=5)\n",
    "print \"Stdv: \", np.round(X_pca.std(axis=0), decimals=5)\n",
    "print \"Corr: \", np.round(np.corrcoef(X_pca.T), decimals=5)\n",
    "\n",
    "# save pca to df and add back dropped cols\n",
    "df_pca = pd.DataFrame(X_pca,\n",
    "                      columns=['pca{}'.format(c) for c in range(ncomps)],\n",
    "                      index=df_all.index)\n",
    "for c in drop:\n",
    "    df_pca[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "pcacols = [c for c in df_pca.columns if c[:3] == 'pca']\n",
    "\n",
    "# check size\n",
    "assert(df_pca.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot('pca0', 'pca1',\n",
    "               data=df_pca,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_pca', df_pca)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like linearly separable data (as exposed in correlation tasks above). Need to try kernel PCA to remove the nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform X\n",
    "X_kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=20).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save pca to df and add back dropped cols\n",
    "df_kpca = pd.DataFrame(X_kpca,\n",
    "                       columns=['kpca{}'.format(c) for c in range(ncomps)],\n",
    "                       index=df_all.index)\n",
    "\n",
    "for c in drop:\n",
    "    df_kpca[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "kpcacols = [c for c in df_kpca.columns if c[:4] == 'kpca']\n",
    "\n",
    "# check size\n",
    "assert(df_kpca.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot('kpca0', 'kpca1',\n",
    "               data=df_kpca,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_kpca', df_kpca)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "X_lle, err = manifold.locally_linear_embedding(X, n_neighbors=12,\n",
    "                                             n_components=2)\n",
    "print(\"Done. Reconstruction error: %g\" % err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save pca to df and add back dropped cols\n",
    "df_lle = pd.DataFrame(X_lle,\n",
    "                      columns=['lle{}'.format(c) for c in range(ncomps)],\n",
    "                      index=df_all.index)\n",
    "for c in drop:\n",
    "    df_lle[c] = df_all[c]\n",
    "\n",
    "# save col names for easy filter later\n",
    "llecols = [c for c in df_lle.columns if c[:3] == 'lle']\n",
    "\n",
    "# check size\n",
    "assert(df_lle.shape[0] == df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.lmplot('lle0', 'lle1',\n",
    "               data=df_lle,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_lle', df_lle)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried SVD and KPCA inputs: SVD outperformed any other. Using it as input to t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build tsne fitter\n",
    "from sklearn.manifold import TSNE\n",
    "#metric = metrics.pairwise.manhattan_distances\n",
    "tsne = TSNE(n_components=2,\n",
    "            #init='pca',\n",
    "            perplexity=50,\n",
    "            #metric=metric,\n",
    "            learning_rate=500,\n",
    "            method='barnes_hut',\n",
    "            verbose=2)\n",
    "\n",
    "# fit training data only\n",
    "Z = tsne.fit_transform(df_svd[svdcols])\n",
    "\n",
    "# save to df\n",
    "df_tsne = pd.DataFrame(Z, columns=['x','y'], index=df_svd.index)\n",
    "for c in drop:\n",
    "    df_tsne[c] = df_svd[c]\n",
    "\n",
    "# check size\n",
    "assert(df_tsne.shape[0] == df_svd.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kpca\n",
    "g = sns.lmplot('x', 'y',\n",
    "               data=df_tsne,\n",
    "               fit_reg=False,\n",
    "               col=\"fault_severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_tsne', df_tsne)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering may be interesting too: could add cluster membership as a new feature. \n",
    "\n",
    "* [Binary Jaccard similarity matrix fed into hierarchical cluster and then using the top \"nodes\"](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html)\n",
    "* [K-mode/median clustering](https://github.com/nicodv/kmodes)\n",
    "* [Cosine similarities fed into spectral clustering or dbscan](http://stackoverflow.com/questions/30089675/clustering-cosine-similarity-matrix)\n",
    "* Frequent itemset mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the log_feature features which are scaled volumes\n",
    "ecols = [c for c in df_all.columns if 'log_feature' not in c]\n",
    "X = df_all.copy()[ecols]\n",
    "X = X[(X.is_train == True)].drop(drop, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base similarity matrix\n",
    "similarity = np.dot(X, X.T)\n",
    "\n",
    "# squared magnitude of preference vectors (number of occurrences)\n",
    "square_mag = np.diag(similarity)\n",
    "\n",
    "# inverse squared magnitude\n",
    "inv_square_mag = 1 / square_mag\n",
    "\n",
    "# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
    "inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
    "\n",
    "# inverse of the magnitude\n",
    "inv_mag = np.sqrt(inv_square_mag)\n",
    "\n",
    "# cosine similarity\n",
    "cosine = similarity * inv_mag\n",
    "cosine = cosine.T * inv_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find whole factors for matrix\n",
    "factors(cosine.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot it\n",
    "binning = 11\n",
    "plt.figure(figsize=(14, 11))\n",
    "cmap = sns.light_palette(\"blue\", as_cmap=True)\n",
    "plt.imshow(cosine[::binning, ::binning], interpolation='none', cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dbscan expects distances, so need 1-cosine input\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbclust = DBSCAN(min_samples=30, eps=0.6).fit_predict(1.0-cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# viz the cluster dist\n",
    "from collections import Counter\n",
    "for i in Counter(dbclust).items():\n",
    "    print i\n",
    "sns.distplot(dbclust, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### K-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kmodes import kmodes\n",
    "\n",
    "# build the clusters\n",
    "km = kmodes.KModes(n_clusters=22, init='Cao', n_init=5, verbose=1)\n",
    "clusters = km.fit_predict(X)\n",
    "\n",
    "# viz the cluster dist\n",
    "for i in Counter(clusters).items():\n",
    "    print i\n",
    "sns.distplot(clusters, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a terrible result, it somewhat mimics the tsne result, except for bins 3 and 4, as I don't see any isolated points in tsne, but it's not a 1:1 comparison. Let's see how this performs. First add the cluster assignments to the X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "revisit the correlation measures\n",
    "run a hierarch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# create dist matrix\n",
    "Y = 1.0 - np.abs(df_svd.corr())\n",
    "Z = linkage(Y, 'complete', 'correlation')\n",
    "\n",
    "# check cophenetic coeffs; closer to 1, the better\n",
    "c, coph_dists = cophenet(Z, Y)\n",
    "print c\n",
    "\n",
    "# view dendro\n",
    "dendro = dendrogram(z, labels=Y.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "plt.figure(figsize=(14, 11))\n",
    "plt.scatter(df_svd[:, 0], df_svd[:, 1], c=clusters, cmap='prism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check validity of clusters using permutation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create permutation\n",
    "svm = SVC(kernel='linear')\n",
    "cv = StratifiedKFold(y, 2)\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    svm, X, y, scoring=\"accuracy\", cv=cv, n_permutations=100, n_jobs=-1)\n",
    "print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))\n",
    "\n",
    "# histo of permu scores\n",
    "plt.hist(permutation_scores, 20, label='Permutation scores')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='Classification Score (pvalue %s)' % pvalue)\n",
    "plt.plot(2 * [1. / n_classes], ylim, '--k', linewidth=3, label='Luck')\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append cluster membership to the no-location version of X\n",
    "a = np.array(clusters)[np.newaxis]\n",
    "print 'X no loc ', X_nl.shape\n",
    "X_nlc = np.concatenate((X_nl, a.T), axis=1)\n",
    "print 'X no loc+clu ', X_nlc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X = \n",
    "y = \n",
    "\n",
    "# pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# univariate\n",
    "selection = SelectKBest(k=1)\n",
    "\n",
    "# combined estimator\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "\n",
    "# Do grid search over k, n_components and C:\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n",
    "param_grid = dict(features__pca__n_components=[1, 2, 3],\n",
    "                  features__univ_select__k=[1, 2],\n",
    "                  svm__C=[0.1, 1, 10])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lasagne (Theano)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use PCA with NN (2 hidden layers + ReLU + dropout)\n",
    "* shooting for logloss < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "# real train data\n",
    "X = df_all[(df_all.is_train==True)].copy()\n",
    "y = X.fault_severity.values\n",
    "X.drop(drop, 1, inplace=True)\n",
    "X = X.as_matrix()\n",
    "\n",
    "# theano format\n",
    "X = X.astype(thno.config.floatX)\n",
    "y = y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input layer\n",
    "l_in = lasagne.layers.InputLayer(\n",
    "    shape=X.shape\n",
    ")\n",
    "\n",
    "# hidden layer1\n",
    "l_hidden1 = lasagne.layers.DenseLayer(\n",
    "    l_in,\n",
    "    num_units=30,\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.HeNormal(gain='relu')\n",
    ")\n",
    "\n",
    "# hidden layer2\n",
    "l_hidden2 = lasagne.layers.DenseLayer(\n",
    "    l_hidden1,\n",
    "    num_units=10,\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.HeNormal(gain='relu')\n",
    ")\n",
    "\n",
    "# dropout layer\n",
    "l_dropout = lasagne.layers.DropoutLayer(\n",
    "    l_hidden2,\n",
    "    p=0.25\n",
    ")\n",
    "\n",
    "# output layer\n",
    "l_output = lasagne.layers.DenseLayer(\n",
    "    l_dropout,\n",
    "    num_units=3,\n",
    "    nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_train = lasagne.layers.get_output(l_output, X)\n",
    "\n",
    "loss_train = lasagne.objectives.aggregate(\n",
    "    lasagne.objectives.categorical_crossentropy(output_train, y))\n",
    "\n",
    "output_eval = lasagne.layers.get_output(l_output, X, deterministic=True)\n",
    "\n",
    "loss_train = lasagne.objectives.aggregate(\n",
    "    lasagne.objectives.categorical_crossentropy(output_eval, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_params = lasagne.layers.get_all_params(l_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use ADADELTA for updates\n",
    "updates = lasagne.updates.adadelta(loss_train, all_params)\n",
    "true_output = T.ivector('true_output')\n",
    "train = theano.function([l_in.input_var, true_output], loss_train, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train in mini-batches\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "\n",
    "# iterate\n",
    "batch_idx = 0\n",
    "epoch = 0\n",
    "while epoch < n_epochs:\n",
    "    # extract train data/label batch and update the parameters with it\n",
    "    train(dataset['train']['X'][batch_idx:batch_idx + batch_size],\n",
    " \n",
    "          \n",
    "          \n",
    "          dataset['train']['y'][batch_idx:batch_idx + BATCH_SIZE])\n",
    " batch_idx += BATCH_SIZE\n",
    " # Once we've trained on the entire training set...\n",
    " if batch_idx >= dataset['train']['X'].shape[0]:\n",
    " # Reset the batch index\n",
    " batch_idx = 0\n",
    " # Update the number of epochs trained\n",
    " epoch += 1\n",
    " # Compute the network's on the validation data\n",
    " val_output = get_output(dataset['valid']['X'])\n",
    " # The predicted class is just the index of the largest probability in the output\n",
    " val_predictions = np.argmax(val_output, axis=1)\n",
    " # The accuracy is the average number of correct predictions\n",
    " accuracy = np.mean(val_predictions == dataset['valid']['y'])\n",
    " print(\"Epoch {} validation accuracy: {}\".format(epoch, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Keras (Theano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# quick existing model test\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "thno.config.compute_test_value = 'warn'\n",
    "\n",
    "# base data\n",
    "X_train = df_train.copy().drop(drop, 1).as_matrix()\n",
    "y_train = df_train.fault_severity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rebuild the train and test sets\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)\n",
    "Y_train = np_utils.to_categorical(y_train, 3)\n",
    "Y_test = np_utils.to_categorical(y_test, 3)\n",
    "\n",
    "# common vars\n",
    "nb_epoch = 100\n",
    "dims = X_train.shape[1]\n",
    "fs = list(factors(dims))\n",
    "batch_size = fs[-1]/4\n",
    "\n",
    "# NN model and layers\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(dims,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# build model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          show_accuracy=True,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "                       show_accuracy=True,\n",
    "                       verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base data\n",
    "X = df_test.copy()\n",
    "ids = X.id\n",
    "for c in drop:\n",
    "    if c in X.columns:\n",
    "        X.drop(c, 1, inplace=True)\n",
    "X = X.as_matrix()\n",
    "y_probs = model.predict(X)\n",
    "\n",
    "final = [[iden, (y_probs[j][0]), (y_probs[j][1]), (y_probs[j][2])] for j, iden in enumerate(ids)]\n",
    "frame = pd.DataFrame(final,columns=('id','predict_0','predict_1','predict_2'))\n",
    "frame.to_csv(\"keras.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Run|Local  |LB     |Changes|\n",
    "|---|-------|-------|:------|\n",
    "|1  |0.62779|0.65748|Initial\n",
    "|2  |0.67230|-------|Adadelta opt\n",
    "|3  |0.75352|-------|SGD opt\n",
    "|4  |0.82378|-------|Adam\n",
    "\n",
    "\n",
    "According to forums:\n",
    "\"The norm for CV and LB disagreement seems to be about 0.01-0.03 based on what I've experienced and read from others. A disagreement as large as you have indicates you're probably over-fitting and may have some minor leakage.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get train inputs\n",
    "X_train = df_train.copy()\n",
    "y_train = X_train.fault_severity\n",
    "X_train.drop(drop, 1, inplace=True)\n",
    "\n",
    "# get test inputs\n",
    "X_test = df_test.copy()\n",
    "X_test.drop(drop, 1, inplace=True)\n",
    "\n",
    "# split train into... train and test\n",
    "from sklearn.cross_validation import train_test_split\n",
    "xg_train, xg_test, yg_train, yg_test = train_test_split(X_train, y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_train = xgb.DMatrix(xg_train, label=yg_train.values)\n",
    "xg_test = xgb.DMatrix(xg_test, label=yg_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xg_train.save_binary(\"xg_train.buffer\")\n",
    "xg_test.save_binary(\"xg_test.buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nclass = 3\n",
    "param = {'booster':'gbtree',\n",
    "         'objective': 'multi:softprob',\n",
    "         'num_class': nclass,\n",
    "         'eval_metric': 'mlogloss',\n",
    "         'max_depth': 0,\n",
    "         'subsample': 0.9,\n",
    "         'colsample_bytree': 0.9,\n",
    "         'eta': 0.0,\n",
    "         'gamma': 0.0,\n",
    "         #'alpha': 0.1,\n",
    "         #'lambda': 0.1,\n",
    "         'silent': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MyXGBClassifier(n_rounds=n_rounds,eta=0.2,max_depth=10,subsample=0.9,colsample_bytree=0.9)\n",
    "gridsearch = GridSearchCV(clf, params, scoring=scorer_logloss, n_jobs=10, cv=3, refit=True)\n",
    "gridsearch.fit(X,Y)\n",
    "\n",
    "grid_scores = gridsearch.grid_scores_\n",
    "top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:3]\n",
    "for i, score in enumerate(top_scores):\n",
    "    print(\"Model with rank: {0}\".format(i + 1))\n",
    "    print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "            score.mean_validation_score,\n",
    "            np.std(score.cv_validation_scores)))\n",
    "    print(\"Parameters: {0}\".format(score.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_round = 1000\n",
    "best_perr = 1.0\n",
    "for gamma in (0, 0.5, 1):\n",
    "    for depth in range(5, 20, 5):\n",
    "        for eta in np.arange(0.1, 0.22, 0.02):\n",
    "            param['max_depth'] = depth\n",
    "            param['eta'] = eta\n",
    "            param['gamma'] = gamma\n",
    "            bst = xgb.train(param, xg_train, num_round)\n",
    "            yg_prob = bst.predict(xg_test).reshape(yg_test.shape[0], nclass)\n",
    "            yg_label = np.argmax(yg_prob, axis=1)\n",
    "            perr = sum(int(yg_label[i]) != yg_test.values[i] for i in\n",
    "                       range(len(yg_test.values))) / float(len(yg_test.values))\n",
    "            print \"Gamma: %.3f, Depth: %d, Eta: %.3f, Err: %.3f\" % (gamma, depth, eta, perr)\n",
    "            if perr < best_perr:\n",
    "                print '**** NEW BEST *****'\n",
    "                best_perr = perr\n",
    "                best_depth = depth\n",
    "                best_eta = eta\n",
    "                best_gamma = gamma\n",
    "print \"Best Pred. %.3f (Depth %d, Eta %.3f, Gamma %.3f)\" % (best_perr, best_depth, best_eta, best_gamma)\n",
    "param['max_depth'] = best_depth\n",
    "param['eta'] = best_eta\n",
    "param['gamma'] = best_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrain best model\n",
    "bst = xgb.train(param, xg_train, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst.save_model('0001.model')\n",
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "f, ax = plt.subplots(figsize=(8,20))\n",
    "xgb.plot_importance(bst, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on full set\n",
    "X_train = xgb.DMatrix(X_train, label=y_train.values)\n",
    "bst = xgb.train(param, X_train, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup test set\n",
    "X_test = xgb.DMatrix(X_test)\n",
    "y_pred = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = df_all[(df_all.is_train == False)].copy()\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,predict_0,predict_1,predict_2\\n\")\n",
    "    for i, item in enumerate(ypred):\n",
    "        f.write(str(int(dtest.id.iloc[i])) + \",\" + \",\".join(map(lambda x:str(x), item)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_xgb = x_train.as_matrix()\n",
    "y_train_xgb = y_train.as_matrix()\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(x_train_xgb, y_train_xgb)\n",
    "\n",
    "y_probs = model.predict_proba(x_test.as_matrix())\n",
    "print(y_probs)\n",
    "\n",
    "final = [[iden,(y_probs[j][0]),(y_probs[j][1]),(y_probs[j][2])] for j,iden in enumerate(testUniqueIds)]\n",
    "frame = pd.DataFrame(final,columns=('id','predict_0','predict_1','predict_2'))\n",
    "frame.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to build test harness of test, train, and validation splits using KFolding. Then run predicitions for each fold. Also want to keep track of true positive rate (tpr) and false positive rate (fpr) so I can evaluate receiving operating characteristic curves (ROC's) and the associated area under the curve (AUC) of each. I will also plot the confusion matrix for each model to visualize the predicitve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross-validation and confusion matrix helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a cross-validation function\n",
    "@mem.cache\n",
    "def run_cv(X, y, clf_class, **kwargs):\n",
    "    kf = cross_validation.KFold(len(y), n_folds=10, shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    mean_acc = 0.0\n",
    "    mean_prec = 0.0\n",
    "    mean_recl = 0.0\n",
    "    mean_f1 = 0.0\n",
    "    for train_index, test_index in kf:\n",
    "        clf = clf_class(**kwargs)\n",
    "        X_train, X_test = X.ix[train_index], X.ix[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "        mean_acc += metrics.accuracy_score(y[test_index], y_pred[test_index])\n",
    "        mean_recl += metrics.recall_score(y[test_index], y_pred[test_index])\n",
    "        mean_prec += metrics.precision_score(y[test_index], y_pred[test_index])\n",
    "        mean_f1 += metrics.f1_score(y[test_index], y_pred[test_index])\n",
    "    mean_acc /= len(kf)\n",
    "    mean_recl /= len(kf)\n",
    "    mean_prec /= len(kf)\n",
    "    mean_f1 /= len(kf)\n",
    "    return y_pred, mean_acc, mean_prec, mean_recl, mean_f1\n",
    "\n",
    "def draw_confusion_matrices(confusion_matrices, class_names):\n",
    "    labels = list(class_names)\n",
    "    for cm in confusion_matrices:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm[1])\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + labels)\n",
    "        ax.set_yticklabels([''] + labels)\n",
    "        plt.title('%s' % cm[0])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        for i,j in ((x,y) for x in xrange(len(cm[1])) for y in xrange(len(cm[1][0]))):\n",
    "            val = '{:.1%}'.format(cm[1][i][j]/cm[1].sum())\n",
    "            ax.annotate(str(val), xy=(i,j), color='white', ha=\"center\", va=\"center\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading up a bunch of standard machine learning algorithms into test harness. This is a classification problem, so let's try the standard models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "ml_models = []\n",
    "ml_models.append([\"Support Vector\", SVC, {'verbose':1}])\n",
    "ml_models.append([\"Logistic Regression\", LR, {'multi_class':'multinomial',\n",
    "                                              'solver':'lbfgs',\n",
    "                                              'verbose':1}])\n",
    "ml_models.append([\"Naive Bayes\", NB, {'verbose':1}])\n",
    "ml_models.append([\"Decision Trees\", DT, {'verbose':1}])\n",
    "ml_models.append([\"Random Forest\", RF, {'n_estimators':200,\n",
    "                                        'min_samples_split':2,\n",
    "                                        'n_jobs':-1,\n",
    "                                        'verbose':1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Measure Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show me the accuracy (acc), precision (prec), recall (recl), confusion matrix (cm), and mean auc for each model. \n",
    "\n",
    "**TODO:**\n",
    "* Gut check accuracy for equivalence:\n",
    "    ```python\n",
    "    metrics.jaccard_similarity_score(y_true, y_pred) == metrics.accuracy_score(y_true, y_pred)\n",
    "    metrics.classification_report(y, y_pred)\n",
    "    metrics.confusion_matrix(y, y_pred)\n",
    "    ``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process each model\n",
    "models = []\n",
    "acc = []\n",
    "prec = []\n",
    "recl = []\n",
    "f1 = []\n",
    "cms = []\n",
    "for m in ml_models:\n",
    "    y_pred, mean_acc, mean_prec, mean_recl, mean_f1 = run_cv(X, y, m[1], **m[2])\n",
    "    models.append(m[0])\n",
    "    acc.append(mean_acc)\n",
    "    prec.append(mean_prec)\n",
    "    recl.append(mean_recl)\n",
    "    f1.append(mean_f1)\n",
    "    cms.append((m[0], metrics.confusion_matrix(y, y_pred)))\n",
    "\n",
    "# plot confusion matricies\n",
    "draw_confusion_matrices(cms, np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame.from_items([('model', models),\n",
    "                              ('accuracy', acc),\n",
    "                              ('precision', prec),\n",
    "                              ('recall', recl),\n",
    "                              ('f1', f1)\n",
    "                              ])\n",
    "models_df.sort(['accuracy', 'precision', 'recall', 'f1'], ascending=[False, False, False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which of the harness models has performed the best in the first pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best and worst models\n",
    "max_idx, max_value = max(enumerate(acc), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best accuracy:', ml_models[max_idx][0], acc[max_idx])\n",
    "\n",
    "max_idx, max_value = max(enumerate(prec), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best precision:', ml_models[max_idx][0], prec[max_idx])\n",
    "\n",
    "max_idx, max_value = max(enumerate(recl), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best recall:', ml_models[max_idx][0], recl[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is the obvious winner by all measures, so let's take a crack at tuning that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Algorithm Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Question: What are the important features in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "kf = cross_validation.KFold(len(y), n_folds=10, shuffle=True, random_state=4)\n",
    "y_pred = y.copy()\n",
    "\n",
    "# keep values for all features\n",
    "feat_impt = {}\n",
    "for f in features:\n",
    "    feat_impt[f] = []\n",
    "\n",
    "# go through all features    \n",
    "for train_index, test_index in kf:\n",
    "    clf = RF(n_estimators=50, min_samples_split=2, n_jobs=-1)\n",
    "    X_train, X_test = X.ix[train_index], X.ix[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test_index] = clf.predict(X_test)\n",
    "    importances = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for f in range(X.shape[1]):\n",
    "        feat_impt[features[indices[f]]].append(importances[indices[f]])\n",
    "\n",
    "for k in feat_impt:\n",
    "    val = np.mean(feat_impt[k])\n",
    "    std = np.std(feat_impt[k])\n",
    "    feat_impt[k] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the features by importance\n",
    "sorted_feat = sorted(feat_impt.items(), key=operator.itemgetter(1))\n",
    "sorted_feat.reverse()\n",
    "\n",
    "# bookkeeping\n",
    "cumper = 0.\n",
    "nbr = False\n",
    "\n",
    "# make a copy of original dataframe to drop unneeded features\n",
    "X_keep = X.copy()\n",
    "\n",
    "# print out importances\n",
    "print \"item -- feature -- weight -- cumm_weight\"\n",
    "for i, k in enumerate(sorted_feat):\n",
    "    cumper += k[1]\n",
    "    \n",
    "    # tell me when we hit 90% of weights\n",
    "    if (nbr is False and cumper >= 0.95):\n",
    "        print '**----  Reached 95%  ----**'\n",
    "        nbr = True\n",
    "    print i, k[0], '{:.1%}'.format(k[1]), '{:.0%}'.format(cumper)\n",
    "    \n",
    "    # drop unneeded features\n",
    "    if nbr is True:\n",
    "        X_keep = X_keep.drop(k[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_keep = X_keep.columns\n",
    "\n",
    "# checks\n",
    "print \"\\n%d observations of %d features\" % X_keep.shape\n",
    "print \"Unique labels:\", np.unique(y)\n",
    "print \"Gut-check features: %i\" % len(features_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the ML test harness for RF only and see if the model improves (below is cut and paste of earlier code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_models = []\n",
    "ml_models.append([\"Random Forest\", RF, {'n_estimators':72,\n",
    "                                        'min_samples_split':9,\n",
    "                                        'n_jobs':-1}])\n",
    "acc = []\n",
    "prec = []\n",
    "recl = []\n",
    "f1 = []\n",
    "cms = []\n",
    "for m in ml_models:\n",
    "    y_pred, mean_acc, mean_prec, mean_recl, mean_f1 = run_cv(X_keep, y, m[1], **m[2])\n",
    "    acc.append(mean_acc)\n",
    "    prec.append(mean_prec)\n",
    "    recl.append(mean_recl)\n",
    "    f1.append(mean_f1)\n",
    "    cms.append((m[0], metrics.confusion_matrix(y, y_pred)))\n",
    "\n",
    "draw_confusion_matrices(cms, np.unique(y))\n",
    "max_idx, max_value = max(enumerate(acc), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best accuracy:', ml_models[max_idx][0], acc[max_idx])\n",
    "max_idx, max_value = max(enumerate(prec), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best precision:', ml_models[max_idx][0], prec[max_idx])\n",
    "max_idx, max_value = max(enumerate(recl), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best recall:', ml_models[max_idx][0], recl[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF predictive power got worse when features contributing < 10% weight were dropped.\n",
    "* **RF predictive power got better when features contributing < 5% weight were dropped.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Question: What model has best calibration and discrimination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration measures the difference between actual and predicted probability for individual groups. Discrimination measures the difference between model predictions and the baseline probability. Equations are taken from [Yang, Yates, and Smith (1991)](http://psychology.huji.ac.il/.upload/Ilan/YanivYatesSmith1991PB.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_prob_cv(X, y, clf_class, **kwargs):\n",
    "    kf = cross_validation.KFold(len(y), n_folds=10, shuffle=True)\n",
    "    y_prob = np.zeros((len(y), 3))\n",
    "    for train_index, test_index in kf:\n",
    "        clf = clf_class(**kwargs)\n",
    "        X_train, X_test = X.ix[train_index], X.ix[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "    return y_prob\n",
    "\n",
    "def calibration(prob, outcome,n_bins=10):\n",
    "    prob = np.array(prob)\n",
    "    outcome = np.array(outcome)\n",
    "    c = 0.0\n",
    "    judgement_bins = np.arange(n_bins + 1) / n_bins\n",
    "    bin_num = np.digitize(prob,judgement_bins)\n",
    "    for j_bin in np.unique(bin_num):\n",
    "        in_bin = bin_num == j_bin\n",
    "        predicted_prob = np.mean(prob[in_bin])\n",
    "        true_bin_prob = np.mean(outcome[in_bin])\n",
    "        c += np.sum(in_bin) * ((predicted_prob - true_bin_prob) ** 2)\n",
    "    return c / len(prob)\n",
    "\n",
    "def discrimination(prob, outcome, n_bins=10):\n",
    "    prob = np.array(prob)\n",
    "    outcome = np.array(outcome)\n",
    "    d = 0.0\n",
    "    base_prob = np.mean(outcome)\n",
    "    judgement_bins = np.arange(n_bins + 1) / n_bins\n",
    "    bin_num = np.digitize(prob,judgement_bins)\n",
    "    for j_bin in np.unique(bin_num):\n",
    "        in_bin = bin_num == j_bin\n",
    "        true_bin_prob = np.mean(outcome[in_bin])\n",
    "        d += np.sum(in_bin) * ((true_bin_prob - base_prob) ** 2)\n",
    "    return d / len(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Lower calibration and higher discrimination are preferred...\"\n",
    "\n",
    "ml_models = []\n",
    "#ml_models.append([\"XGBoost\", XGBC, {'max_depth':3, 'n_estimators':300, 'learning_rate':0.05}])\n",
    "#ml_models.append([\"Support Vector Machine\", SVC, {}])\n",
    "ml_models.append([\"Linear Logistic Regression\", LR, {'multi_class':'multinomial',\n",
    "                                                     'solver':'lbfgs'}])\n",
    "ml_models.append([\"Naive Bayes\", NB, {}])\n",
    "ml_models.append([\"Decision Trees\", DT, {}])\n",
    "ml_models.append([\"Random Forest\", RF, {'n_estimators':200,\n",
    "                                        'min_samples_split':2, 'n_jobs':-1}])\n",
    "\n",
    "cal_err = []\n",
    "discrim = []\n",
    "for m in ml_models:\n",
    "    print \"\\n\"+m[0]\n",
    "    pred_prob = run_prob_cv(X, y, m[1], **m[2])\n",
    "    churn_prob, is_churn = pred_prob[:,1], y == 1\n",
    "    cal_err.append(calibration(churn_prob, is_churn))\n",
    "    discrim.append(discrimination(churn_prob,is_churn))\n",
    "    print '{:20} {:.4f}'.format(\"Calibration Error\", cal_err[-1])\n",
    "    print '{:20} {:.4f}'.format(\"Discrimination\", discrim[-1])\n",
    "    \n",
    "idx, value = min(enumerate(cal_err), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.3%}'.format('\\nBest calibration error:', ml_models[idx][0], cal_err[idx])\n",
    "idx, value = max(enumerate(discrim), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best discrimination:', ml_models[idx][0], discrim[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Question: Can we tune the model params to get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RF(n_jobs=-1)\n",
    "param_grid = {\"n_estimators\":[50, 75, 100],\n",
    "              \"max_depth\": [None, 3],\n",
    "              \"max_features\": [None, 'auto'],\n",
    "              \"min_samples_split\": [4, 9],\n",
    "              \"min_samples_leaf\": [1],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "gs = grid_search.GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(gs.grid_scores_)))\n",
    "report(grid_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best scores using X (took 542.40 seconds for 32 candidate parameter settings):**\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.956 (std: 0.007)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 500, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.956 (std: 0.009)\n",
    "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.955 (std: 0.006)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "\n",
    "**Best scores using X_keep (took 471.68 seconds for 48 candidate parameter settings):**\n",
    "\n",
    "Model with rank: 1\n",
    "Mean validation score: 0.957 (std: 0.007)\n",
    "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 1000, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.956 (std: 0.006)\n",
    "Parameters: {'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 500, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.956 (std: 0.007)\n",
    "Parameters: {'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 50, 'min_samples_split': 4, 'criterion': 'entropy', 'max_features': 'auto', 'max_depth': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_models = []\n",
    "ml_models.append([\"Random Forest\", RF, {'bootstrap': True,\n",
    "                                        'min_samples_leaf': 1,\n",
    "                                        'n_estimators': 500,\n",
    "                                        'min_samples_split': 4,\n",
    "                                        'criterion': 'entropy',\n",
    "                                        'max_features': 'auto',\n",
    "                                        'max_depth': None,\n",
    "                                        'n_jobs': -1}])\n",
    "acc = []\n",
    "prec = []\n",
    "recl = []\n",
    "f1 = []\n",
    "cms = []\n",
    "for m in ml_models:\n",
    "    y_pred, mean_acc, mean_prec, mean_recl, mean_f1 = run_cv(X, y, m[1], **m[2])\n",
    "    acc.append(mean_acc)\n",
    "    prec.append(mean_prec)\n",
    "    recl.append(mean_recl)\n",
    "    f1.append(mean_f1)\n",
    "    cms.append((m[0], metrics.confusion_matrix(y, y_pred)))\n",
    "draw_confusion_matrices(cms, np.unique(y))\n",
    "max_idx, max_value = max(enumerate(acc), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best accuracy:', ml_models[max_idx][0], acc[max_idx])\n",
    "max_idx, max_value = max(enumerate(prec), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best precision:', ml_models[max_idx][0], prec[max_idx])\n",
    "max_idx, max_value = max(enumerate(recl), key=operator.itemgetter(1))\n",
    "print '{:20} {:20} {:.2%}'.format('Best recall:', ml_models[max_idx][0], recl[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is hardly better than the initial guess, but it's technically simpler, so I'll run with this one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use 10 estimators so predictions are all multiples of 0.1\n",
    "m = ([\"Random Forest\", RF, {'bootstrap': True,\n",
    "                            'min_samples_leaf': 1,\n",
    "                            'n_estimators': 10,\n",
    "                            'min_samples_split': 1,\n",
    "                            'criterion': 'entropy',\n",
    "                            'max_features': 'auto',\n",
    "                            'max_depth': None,\n",
    "                            'n_jobs': -1}])\n",
    "pred_prob = run_prob_cv(X, y, m[1], **m[2])\n",
    "pred_churn = pred_prob[:,1]\n",
    "is_churn = y == 1\n",
    "\n",
    "# Number of times a predicted probability is assigned to an observation\n",
    "counts = pd.value_counts(pred_churn)\n",
    "\n",
    "# calculate true probabilities\n",
    "true_prob = {}\n",
    "for prob in counts.index:\n",
    "    true_prob[prob] = np.mean(is_churn[pred_churn == prob])\n",
    "    true_prob = pd.Series(true_prob)\n",
    "\n",
    "# pandas-fu\n",
    "counts = pd.concat([counts,true_prob], axis=1).reset_index()\n",
    "counts.columns = ['pred_prob', 'count', 'true_prob']\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "baseline = np.mean(is_churn)\n",
    "ggplot(counts, aes(x='pred_prob',y='true_prob',size='count')) + \\\n",
    "        geom_point(color='blue') + \\\n",
    "        stat_function(fun = lambda x: x, color='red') + \\\n",
    "        stat_function(fun = lambda x: baseline, color='green') + \\\n",
    "        xlim(-0.05,  1.05) + \\\n",
    "        ylim(-0.05,1.05) + \\\n",
    "        ggtitle(\"Random Forest\") + \\\n",
    "        xlab(\"Predicted probability\") + ylab(\"Relative frequency of outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# event, feature, resource, severity, volume, location\n",
    "\n",
    "adc = pd.read_csv('adc.csv') #efrs\n",
    "rf = pd.read_csv('randomforest.csv') #esv\n",
    "loc = pd.read_csv('location.csv') # l\n",
    "gbc = pd.read_csv('gbc.csv') # e\n",
    "nn = pd.read_csv('neural_network.csv') # rs\n",
    "xgbc = pd.read_csv('xgbc.csv') # efrsvl\n",
    "\n",
    "df_final = rf.copy()\n",
    "df_final.loc[:, 'predict_0':'predict_2'] = (\n",
    "    5.0*rf.loc[:, 'predict_0':'predict_2'] + \n",
    "    3.0*loc.loc[:, 'predict_0':'predict_2'] +\n",
    "    0.1*adc.loc[:, 'predict_0':'predict_2'] +\n",
    "    0.2*nn.loc[:, 'predict_0':'predict_2'] +\n",
    "    6.0*gbc.loc[:, 'predict_0':'predict_2'] +\n",
    "    10.0*xgbc.loc[:, 'predict_0':'predict_2']\n",
    "    )/24.3\n",
    "\n",
    "df_final.to_csv('blend.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to hdf5 file\n",
    "hdf = pd.HDFStore(hdf_file)\n",
    "hdf.put('df_final', df_final)\n",
    "hdf.close()\n",
    "call([\"lrztar\", \"-zf\", hdf_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ad Hoc Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build random forest\n",
    "clf = RandomForestClassifier(n_estimators=50, oob_score=True)  \n",
    "n_trials = 100\n",
    "test_size_percent = 0.1\n",
    "\n",
    "# build labels\n",
    "sig_field = 'sms_count'\n",
    "signals = df[[c for c in df.columns if c != sig_field]]\n",
    "labels = df[sig_field]\n",
    "\n",
    "# test train split\n",
    "plot_data = []\n",
    "for trial in range(n_trials):\n",
    "    train_signals, test_signals, train_labels, test_labels = train_test_split(\n",
    "                                                            signals, labels, test_size=test_size_percent)\n",
    "    clf.fit(train_signals, train_labels)\n",
    "    predictions = clf.predict_proba(test_signals)[:,1]\n",
    "    precision, recall, thresholds = precision_recall_curve(test_labels, predictions)  \n",
    "    thresholds = np.append(thresholds, 1)\n",
    "    queue_rate = []  \n",
    "    for threshold in thresholds:  \n",
    "        queue_rate.append((predictions >= threshold).mean())\n",
    "        \n",
    "    # save plot data\n",
    "    plot_data.append({\n",
    "            'thresholds': thresholds\n",
    "        ,   'precision': precision\n",
    "        ,   'recall': recall\n",
    "        ,   'queue_rate': queue_rate\n",
    "    })\n",
    "    \n",
    "# plot model\n",
    "uniform_thresholds = np.linspace(0, 1, 101)\n",
    "uniform_precision_plots = []  \n",
    "uniform_recall_plots= []  \n",
    "uniform_queue_rate_plots= []\n",
    "for p in plot_data:  \n",
    "    uniform_precision = []\n",
    "    uniform_recall = []\n",
    "    uniform_queue_rate = []\n",
    "    for ut in uniform_thresholds:\n",
    "        index = bisect.bisect_left(p['thresholds'], ut)\n",
    "        uniform_precision.append(p['precision'][index])\n",
    "        uniform_recall.append(p['recall'][index])\n",
    "        uniform_queue_rate.append(p['queue_rate'][index])\n",
    "\n",
    "    uniform_precision_plots.append(uniform_precision)\n",
    "    uniform_recall_plots.append(uniform_recall)\n",
    "    uniform_queue_rate_plots.append(uniform_queue_rate)\n",
    "\n",
    "quantiles = [0.1, 0.5, 0.9]  \n",
    "lower_precision, median_precision, upper_precision = mstats.mquantiles(uniform_precision_plots, quantiles, axis=0)  \n",
    "lower_recall, median_recall, upper_recall = mstats.mquantiles(uniform_recall_plots, quantiles, axis=0)  \n",
    "lower_queue_rate, median_queue_rate, upper_queue_rate = mstats.mquantiles(uniform_queue_rate_plots, quantiles, axis=0)\n",
    "\n",
    "plt.plot(uniform_thresholds, median_precision)  \n",
    "plt.plot(uniform_thresholds, median_recall)  \n",
    "plt.plot(uniform_thresholds, median_queue_rate)\n",
    "\n",
    "plt.fill_between(uniform_thresholds, upper_precision, lower_precision, alpha=0.5, linewidth=0, color=sns.color_palette()[0])  \n",
    "plt.fill_between(uniform_thresholds, upper_recall, lower_recall, alpha=0.5, linewidth=0, color=sns.color_palette()[1])  \n",
    "plt.fill_between(uniform_thresholds, upper_queue_rate, lower_queue_rate, alpha=0.5, linewidth=0, color=sns.color_palette()[2])\n",
    "\n",
    "leg = plt.legend(('precision', 'recall', 'queue_rate'), frameon=True)  \n",
    "leg.get_frame().set_edgecolor('k')  \n",
    "plt.xlabel('threshold')  \n",
    "plt.ylabel('%')  \n",
    "\n",
    "uniform_payout_plots = []\n",
    "n = 10000  \n",
    "success_payoff = 100  \n",
    "case_cost = 20\n",
    "\n",
    "for p in plot_data:  \n",
    "    uniform_payout = []\n",
    "    for ut in uniform_thresholds:\n",
    "        index = bisect.bisect_left(p['thresholds'], ut)\n",
    "        precision = p['precision'][index]\n",
    "        queue_rate = p['queue_rate'][index]\n",
    "        payout = n*queue_rate*(precision*100 - case_cost)\n",
    "        uniform_payout.append(payout)\n",
    "    uniform_payout_plots.append(uniform_payout)\n",
    "\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "lower_payout, median_payout, upper_payout = mstats.mquantiles(uniform_payout_plots, quantiles, axis=0)\n",
    "\n",
    "plt.plot(uniform_thresholds, median_payout, color=sns.color_palette()[4])  \n",
    "plt.fill_between(uniform_thresholds, upper_payout, lower_payout, alpha=0.5, linewidth=0, color=sns.color_palette()[4])\n",
    "\n",
    "max_ap = uniform_thresholds[np.argmax(median_payout)]  \n",
    "plt.vlines([max_ap], -100000, 150000, linestyles='--')  \n",
    "plt.ylim(-100000, 150000)\n",
    "\n",
    "leg = plt.legend(('payout ($)', 'median argmax = {:.2f}'.format(max_ap)), frameon=True)  \n",
    "leg.get_frame().set_edgecolor('k')  \n",
    "plt.xlabel('threshold')  \n",
    "plt.ylabel('$')  \n",
    "plt.title(\"Payout as a Function of Threshold\")\n",
    "\n",
    "print '{0:.0f}'.format(np.max(median_payout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "notes.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
