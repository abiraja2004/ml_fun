{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, GenericUnivariateSelect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('../data/train.csv')\n",
    "test_orig = pd.read_csv('../data/test.csv')\n",
    "event_type = pd.read_csv('../data/event_type.csv')\n",
    "resource_type = pd.read_csv('../data/resource_type.csv')\n",
    "severity_type = pd.read_csv('../data/severity_type.csv')\n",
    "log_feature = pd.read_csv('../data/log_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_log_loss(predictions, answers):\n",
    "    m = len(predictions)\n",
    "    zeros = np.zeros(shape=(m, 3))\n",
    "    y = pd.DataFrame(zeros, columns=[0, 1, 2])\n",
    "    for i, c in enumerate(answers):\n",
    "        y.set_value(i, c, 1)\n",
    "    return -(1/m) * np.sum(np.sum(y * np.log(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(df, base_split=0.20, recombine=0.6):\n",
    "    n_splits = int(math.floor(1/base_split))\n",
    "    nmb_unknown = len(df['location']) * 0.10\n",
    "    locations = df['location'].value_counts()\n",
    "    location_labels = sorted(locations.index.tolist())\n",
    "    unique_location_sets = []\n",
    "    for i in range(n_splits):\n",
    "        ctr = 0\n",
    "        group_labels = []\n",
    "        while ctr < nmb_unknown:\n",
    "            group_index = random.randint(0, len(location_labels)-1)\n",
    "            unknown = location_labels[group_index]\n",
    "            location_labels[group_index], location_labels[-1] = \\\n",
    "                location_labels[-1], location_labels[group_index]\n",
    "            location_labels.pop()\n",
    "            group_labels.append(unknown)\n",
    "            ctr += locations[unknown]\n",
    "        unique_location_sets.append(group_labels)\n",
    "    for unknowns in unique_location_sets:\n",
    "        msk = df['location'].isin(unknowns)\n",
    "        unknown_test_locations = df[msk]\n",
    "        rest = df[~msk]\n",
    "        train = rest.sample(frac=0.44, random_state=42)\n",
    "        rest_test = rest[~rest.index.isin(train.index)]\n",
    "        test = pd.concat([rest_test, unknown_test_locations], axis=0)\n",
    "        yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_on_total(x, distribution, total=1, value='Rare'):\n",
    "    try:\n",
    "        if distribution[x] < total:\n",
    "            return value\n",
    "    except (ValueError, KeyError):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resource_type_features(df):\n",
    "    resource_type_dummies = pd.get_dummies(resource_type['resource_type'])\n",
    "    resource_dummy = pd.concat([resource_type, resource_type_dummies],\n",
    "                               axis=1)\n",
    "    resource_grpd = resource_dummy.groupby(resource_dummy.id).sum()\n",
    "    df_resource = pd.merge(df, resource_grpd, left_on='id', right_index=True)\n",
    "    return df_resource.drop(['resource_type 1', 'resource_type 2',\n",
    "                             'resource_type 4',\n",
    "                             'resource_type 6', 'resource_type 8',\n",
    "                             'resource_type 10'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resource_type_event_count(df):\n",
    "\n",
    "    p = pd.merge(resource_type, event_type, on='id')\n",
    "    t = p.loc[p['resource_type'] == 'resource_type 2', ['id', 'event_type']] \\\n",
    "        .groupby(by=['id'], as_index=False).count()\n",
    "    df_two = pd.merge(df, t, on='id', how='left')\n",
    "\n",
    "    f = p.loc[p['resource_type'] == 'resource_type 4', ['id', 'event_type']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_four = pd.merge(df_two, f, on='id', how='left')\n",
    "\n",
    "    e = p.loc[p['resource_type'] == 'resource_type 8', ['id', 'event_type']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_eight = pd.merge(df_four, e, on='id', how='left')\n",
    "\n",
    "    t = p.loc[p['resource_type'] == 'resource_type 10', ['id', 'event_type']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_ten = pd.merge(df_eight, t, on='id', how='left')\n",
    "\n",
    "    return df_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resource_type_log(df):\n",
    "\n",
    "    p = pd.merge(resource_type, log_feature, on='id')\n",
    "    t = p.loc[(p['resource_type'] == 'resource_type 1'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_one = pd.merge(df, t, on='id', how='left')\n",
    "\n",
    "    tw = p.loc[(p['resource_type'] == 'resource_type 2'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_two = pd.merge(df_one, tw, on='id', how='left')\n",
    "\n",
    "    se = p.loc[(p['resource_type'] == 'resource_type 7'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_se = pd.merge(df_two, se, on='id', how='left')\n",
    "\n",
    "    ei = p.loc[(p['resource_type'] == 'resource_type 8'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_ei = pd.merge(df_se, ei, on='id', how='left')\n",
    "\n",
    "    n = p.loc[(p['resource_type'] == 'resource_type 9'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_n = pd.merge(df_ei, n, on='id', how='left')\n",
    "\n",
    "    return df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resource_type_log_sum(df):\n",
    "\n",
    "    p = pd.merge(resource_type, log_feature, on='id')\n",
    "    t = p.loc[(p['resource_type'] == 'resource_type 3'), ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()\n",
    "    df_one = pd.merge(df, t, on='id', how='left')\n",
    "\n",
    "    f = p.loc[(p['resource_type'] == 'resource_type 4'), ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()\n",
    "    df_f = pd.merge(df, f, on='id', how='left')\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resource_type_log_total(df):\n",
    "\n",
    "    p = pd.merge(resource_type, log_feature, on='id')\n",
    "\n",
    "    # resource type 2\n",
    "    t = p.loc[(p['resource_type'] == 'resource_type 2'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "\n",
    "    t.loc[:, 'volume'] = p.loc[(p['resource_type'] == 'resource_type 2'), ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()['volume']\n",
    "\n",
    "    t.loc[:, 'total log volume two'] = t['volume'] / t['log_feature']\n",
    "\n",
    "    df_one = pd.merge(df, t[['id', 'total log volume two']], on='id', how='left')\n",
    "\n",
    "    # resource type ten\n",
    "    te = p.loc[(p['resource_type'] == 'resource_type 10'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "\n",
    "    te.loc[:, 'volume'] = p.loc[(p['resource_type'] == 'resource_type 10'), ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()['volume']\n",
    "\n",
    "    te.loc[:, 'total log volume ten'] = te['log_feature'] * te['volume']\n",
    "\n",
    "    df_te = pd.merge(df_one, te[['id', 'total log volume ten']], on='id', how='left')\n",
    "\n",
    "    return df_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resource_type_severity(df):\n",
    "\n",
    "    p = pd.merge(resource_type, severity_type, on='id')\n",
    "    p.loc[:, 'resource type severity'] = \\\n",
    "        ((p['resource_type'] == 'resource_type 6') &\n",
    "        (p['severity_type'] == 'severity_type 1')).astype(float)\n",
    "\n",
    "    t = p[['id', 'resource type severity']].groupby(by='id', as_index=False).median()\n",
    "\n",
    "    return pd.merge(df, t, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def severity_type_features(df):\n",
    "\n",
    "    severity_type_dummies = pd.get_dummies(severity_type['severity_type'])\n",
    "    severity_dummy = pd.concat([severity_type, severity_type_dummies],\n",
    "                               axis=1)\n",
    "\n",
    "    severity_grpd = severity_dummy \\\n",
    "        .groupby(severity_dummy.id).sum()\n",
    "\n",
    "    df_severity = pd.merge(df, severity_grpd, left_on='id', right_index=True)\n",
    "    return df_severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def severity_high(df):\n",
    "\n",
    "    p = severity_type[['id', 'severity_type']]\n",
    "\n",
    "    p.loc[:, 'high log severity'] = \\\n",
    "        (p['severity_type'].isin(['severity_type 3',\n",
    "                                   'severity_type 4',\n",
    "                                   'severity_type 5'])).astype(float)\n",
    "    ret = pd.merge(df, p[['id', 'high log severity']], on='id', how='left')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def severity_event(df):\n",
    "\n",
    "    p = pd.merge(severity_type, event_type, on='id')\n",
    "    t = p.loc[(p['severity_type'] == 'severity_type 1'), ['id', 'event_type']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_one = pd.merge(df, t, on='id', how='left')\n",
    "\n",
    "    return df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def severity_log(df):\n",
    "\n",
    "    p = pd.merge(severity_type, log_feature, on='id')\n",
    "\n",
    "    tw = p.loc[(p['severity_type'] == 'severity_type 2'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_tw = pd.merge(df, tw, on='id', how='left')\n",
    "\n",
    "    return df_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def severity_log_sum(df):\n",
    "\n",
    "    p = pd.merge(severity_type, log_feature, on='id')\n",
    "\n",
    "    t = p.loc[(p['severity_type'] == 'severity_type 2'), ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    t.loc[:, 'severity_volume'] = p.loc[(p['severity_type'] == 'severity_type 2'), ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()\n",
    "\n",
    "    t.loc[:, 'severity_log_volume'] = t['severity_volume'] / t['log_feature']\n",
    "    df_one = pd.merge(df, t[['id', 'severity_log_volume']], on='id', how='left')\n",
    "\n",
    "    return df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_features(df):\n",
    "\n",
    "    log_table = pd.pivot_table(\n",
    "        log_feature, values='volume',\n",
    "        index='id', columns='log_feature',\n",
    "        aggfunc=np.sum, fill_value=0\n",
    "    )\n",
    "    df_log = pd.merge(df, log_table, left_on='id', right_index=True)\n",
    "\n",
    "    return df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_feature_volume(df):\n",
    "\n",
    "    g = log_feature[['id', 'volume']].groupby(by='id', as_index=False).sum()\n",
    "    ret_g = pd.merge(df, g, on='id', how='left')\n",
    "\n",
    "    h = log_feature[['id', 'log_feature']].groupby(by='id', as_index=False).count()\n",
    "    i = pd.merge(h, g, on='id', how='left')\n",
    "\n",
    "    i.loc[:, 'feature_by_volume'] = i['log_feature'] * i['volume']\n",
    "    ret = pd.merge(ret_g, i[['id', 'feature_by_volume']], on='id', how='left')\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_feature_prob(train, test, level=0):\n",
    "\n",
    "    t = pd.merge(train[['id', 'fault_severity']], log_feature, on='id',\n",
    "                 how='left').drop_duplicates()\n",
    "\n",
    "    log_given_severity = \\\n",
    "       t.loc[t['fault_severity'] == level, 'log_feature'].value_counts() / \\\n",
    "       t['fault_severity'].value_counts()[level]\n",
    "\n",
    "    severity_prob = \\\n",
    "        t['fault_severity'].value_counts()[level] / len(t)\n",
    "\n",
    "    log_probs = \\\n",
    "        t['log_feature'].value_counts() / len(t)\n",
    "\n",
    "    log_feature_probs = \\\n",
    "        (log_given_severity * severity_prob) / log_probs\n",
    "\n",
    "    prob_df = pd.DataFrame({'probs': log_feature_probs})\n",
    "\n",
    "    p = pd.merge(t, prob_df, left_on='log_feature', right_index=True, how='left')\n",
    "    prob_table = pd.pivot_table(p, values='probs', index='id', columns='log_feature',\n",
    "                                aggfunc=np.mean, fill_value=0)\n",
    "\n",
    "    train = pd.merge(train, prob_table, left_on='id', right_index=True,\n",
    "                     how='left')\n",
    "    test = pd.merge(test, prob_table, left_on='id', right_index=True,\n",
    "                    how='left')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dangerous_log(train, test, level=0):\n",
    "\n",
    "    feature_count = \\\n",
    "        train.loc[\n",
    "            (train['fault_severity'] == level),\n",
    "            ['log_feature', 'volume']\n",
    "        ].groupby(by='log_feature', sort=False).sum()\n",
    "\n",
    "    feature_total = \\\n",
    "        train.loc[\n",
    "            (train['fault_severity'] == level),\n",
    "            'volume'\n",
    "        ].sum()\n",
    "\n",
    "    danger = feature_count / feature_total\n",
    "\n",
    "    train.loc[\n",
    "        (train['fault_severity'] == level) & (train['log_feature'].isin(danger.index)),\n",
    "            'dangerous_log'\n",
    "    ] = danger['volume']\n",
    "\n",
    "    test.loc[\n",
    "        (test['fault_severity'] == level) & (test['log_feature'].isin(danger.index)),\n",
    "            'dangerous_log'\n",
    "    ] = danger['volume']\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def danger_log(train, test):\n",
    "\n",
    "    train, test = dangerous_log(train, test, level=0)\n",
    "    train, test = dangerous_log(train, test, level=1)\n",
    "    train, test = dangerous_log(train, test, level=2)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_features(df):\n",
    "\n",
    "    df_event = event_type_features(df)\n",
    "    df_resource = resource_type_features(df_event)\n",
    "    df_severity = severity_type_features(df_resource)\n",
    "    df_log = log_features(df_severity)\n",
    "\n",
    "    df_log_count = log_feature_volume(df_log)\n",
    "    df_severity_high = severity_high(df_log_count)\n",
    "    df_resource_type = resource_type_event_count(df_severity_high)\n",
    "    df_resource_type_log = resource_type_log(df_resource_type)\n",
    "\n",
    "    df_complete = df_resource_type_log\n",
    "\n",
    "    return df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def location_features(train, test, cutoff=0):\n",
    "\n",
    "    train_locations = train[['location']]\n",
    "    test_locations = test[['location']]\n",
    "\n",
    "    train_locations.loc[:, 'train'] = True\n",
    "    test_locations.loc[:, 'train'] = False\n",
    "\n",
    "    train_distribution = train_locations['location'].value_counts()\n",
    "\n",
    "    locations = pd.concat([train_locations, test_locations])\n",
    "    locations.loc[:, 'location'] = locations['location'] \\\n",
    "        .apply(rare_category, args=(train_distribution, ),\n",
    "               cutoff=cutoff, value='RareLocation')\n",
    "\n",
    "    locations_bin = pd.get_dummies(locations['location'])\n",
    "    locations_dummy = pd.concat([locations, locations_bin], axis=1)\n",
    "\n",
    "    msk = locations_dummy['train']\n",
    "    locations_dummy.drop(['train', 'location'], axis=1, inplace=True)\n",
    "\n",
    "    train_locs = pd.concat([train, locations_dummy[msk]], axis=1)\n",
    "    test_locs = pd.concat([test, locations_dummy[~msk]], axis=1)\n",
    "\n",
    "    return train_locs, test_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dangerous_location(train, test):\n",
    "\n",
    "    danger = \\\n",
    "        (train.loc[(train['fault_severity'] == 2) | (train['fault_severity'] == 1), 'location']).value_counts() / \\\n",
    "        len(train.loc[(train['fault_severity'] == 2) | (train['fault_severity'] == 1), 'location'])\n",
    "\n",
    "    msk = danger >= 0.025\n",
    "\n",
    "    train.loc[train['location'].isin(danger[msk].index), 'dangerous'] = 1\n",
    "    test.loc[test['location'].isin(danger[msk].index), 'dangerous'] = 1\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_type_features(df):\n",
    "\n",
    "    event_type_dummies = pd.get_dummies(event_type['event_type'])\n",
    "    event_dummy = pd.concat([event_type, event_type_dummies], axis=1)\n",
    "\n",
    "    event_grpd = event_dummy \\\n",
    "        .groupby(event_dummy.id).sum()\n",
    "\n",
    "    df_event = pd.merge(df, event_grpd, left_on='id', right_index=True)\n",
    "    return df_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_resource_features(df):\n",
    "\n",
    "    e = pd.merge(event_type, resource_type, on='id')\n",
    "    p = e.loc[e['event_type'] == 'event_type 23', ['id', 'resource_type']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "    df_23 = pd.merge(df, p, on='id', how='left')\n",
    "\n",
    "    return df_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_log_features(df):\n",
    "\n",
    "    l = pd.merge(event_type, log_feature, on='id')\n",
    "    p = l.loc[l['event_type'] == 'event_type 15', ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()\n",
    "\n",
    "    df_15 = pd.merge(df, p[['id', 'volume']], on='id', how='left')\n",
    "\n",
    "    p = l.loc[l['event_type'] == 'event_type 15', ['id', 'log_feature']] \\\n",
    "        .groupby(by='id', as_index=False).count()\n",
    "\n",
    "    df_15c = pd.merge(df_15, p, on='id', how='left')\n",
    "\n",
    "\n",
    "    p = l.loc[l['event_type'] == 'event_type 14', ['id', 'volume']] \\\n",
    "        .groupby(by='id', as_index=False).sum()\n",
    "    df_14 = pd.merge(df_15c, p[['id', 'volume']], on='id', how='left')\n",
    "\n",
    "    return df_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_severity_prob(train, test, level=0):\n",
    "\n",
    "    t = pd.merge(train[['id', 'fault_severity']], event_type, on='id',\n",
    "                 how='left').drop_duplicates()\n",
    "\n",
    "    event_given_severity = \\\n",
    "        t.loc[t['fault_severity'] == level, 'event_type'].value_counts() / \\\n",
    "        len(t.loc[t['fault_severity'] == level])\n",
    "\n",
    "    severity_probs = \\\n",
    "        len(t.loc[t['fault_severity'] == level, :]) / len(t)\n",
    "\n",
    "    event_probs = \\\n",
    "        t['event_type'].value_counts() / len(t['event_type'])\n",
    "\n",
    "    event_severity_probs = \\\n",
    "        (event_given_severity * severity_probs) / event_probs\n",
    "\n",
    "    prob_df = pd.DataFrame({'probs': event_severity_probs})\n",
    "\n",
    "    p = pd.merge(t, prob_df, left_on='event_type', right_index=True, how='left')\n",
    "    prob_table = pd.pivot_table(p, values='probs', index='id', columns='event_type',\n",
    "                                aggfunc=np.median, fill_value=0)\n",
    "\n",
    "    train = pd.merge(train, prob_table, left_on='id', right_index=True,\n",
    "                     how='left')\n",
    "    test = pd.merge(test, prob_table, left_on='id', right_index=True,\n",
    "                    how='left')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_severity(train, test):\n",
    "\n",
    "    train, test = event_severity_prob(train, test, level=2)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rare_category(x, category_distribution, cutoff=1, value='Rare'):\n",
    "    try:\n",
    "        if category_distribution[x] < cutoff:\n",
    "            return value\n",
    "    except (ValueError, KeyError):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgboost_model(train, labels, test):\n",
    "\n",
    "    params = {}\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eval_metric'] = 'mlogloss'\n",
    "    params['eta'] = 0.01\n",
    "    params['gamma'] = 2.0\n",
    "    params['max_depth'] = 8\n",
    "    params['min_child_weight'] = 0.1\n",
    "    params['max_delta_step'] = 1.5\n",
    "    params['subsample'] = 0.75\n",
    "    params['colsample_bytree'] = 0.85\n",
    "    params['lambda'] = 0.2\n",
    "    params['alpha'] = 0\n",
    "    params['silent'] = 1\n",
    "\n",
    "    xgtrain = xgb.DMatrix(train, labels)\n",
    "    xgtest = xgb.DMatrix(test)\n",
    "\n",
    "    num_rounds = 2400\n",
    "    m = xgb.train(list(params.items()), xgtrain, num_rounds)\n",
    "    return m, m.predict(xgtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train_orig\n",
    "test = test_orig\n",
    "\n",
    "train_scores = []\n",
    "scores = []\n",
    "\n",
    "for train, test in train_test(train):\n",
    "    train = base_features(train)\n",
    "    test = base_features(test)\n",
    "    train, test = location_features(train, test, cutoff=5)\n",
    "\n",
    "    labels = train.fault_severity.values\n",
    "    answers = test.fault_severity.values\n",
    "    \n",
    "    train.drop(['id', 'fault_severity', 'location'], axis=1, inplace=True)\n",
    "    test.drop(['id', 'fault_severity', 'location'], axis=1, inplace=True)\n",
    "\n",
    "    train = train.fillna(0)\n",
    "    train = train.astype(float)\n",
    "    test = test.fillna(0)\n",
    "    test = test.astype(float)\n",
    "\n",
    "    chi2 = GenericUnivariateSelect(score_func=chi2,\n",
    "                                   mode='percentile',\n",
    "                                   param=80)\n",
    "    train = chi2.fit_transform(train, labels)\n",
    "    test = chi2.transform(test)\n",
    "\n",
    "    pca = PCA(n_components=400)\n",
    "    train = pca.fit_transform(train)\n",
    "    test = pca.transform(test)\n",
    "    \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=15,\n",
    "        weights='uniform',\n",
    "        leaf_size=50,\n",
    "        p=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    knn.fit(train, labels)\n",
    "    knn_train = np.reshape(knn.predict(train), (-1, 1))\n",
    "    knn_test = np.reshape(knn.predict(test), (-1, 1))\n",
    "    \n",
    "    bayes = MultinomialNB(alpha=0,\n",
    "                          fit_prior=True)\n",
    "    bayes.fit(train, labels)\n",
    "    bayes_train = np.reshape(bayes.predict(train), (-1, 1))\n",
    "    bayes_test = np.reshape(bayes.predict(test), (-1, 1))\n",
    "\n",
    "    svm = LinearSVC(\n",
    "        penalty='l2',\n",
    "        loss='hinge',\n",
    "        C=1.0,\n",
    "        random_state=42,\n",
    "        intercept_scaling=5\n",
    "    )\n",
    "    svm.fit(train, labels)\n",
    "    svm_train = np.reshape(svm.predict(train), (-1, 1))\n",
    "    svm_test = np.reshape(svm.predict(test), (-1, 1))\n",
    "\n",
    "    #train = np.append(train, knn_train, axis=1)\n",
    "    #test = np.append(test, knn_test, axis=1)\n",
    "    #train_labels = np.reshape(labels, (-1, 1))\n",
    "    \n",
    "    #train = np.append(train, train_labels, axis=1)\n",
    "    #test = np.append(test, bayes_test, axis=1)\n",
    "    \n",
    "    #train = np.append(train, train_labels, axis=1)\n",
    "    #test = np.append(test, svm_test, axis=1)\n",
    "    \n",
    "    model, predictions = xgboost_model(train, labels, test)\n",
    "    train_score = sc.multi_log_loss(\n",
    "        model.predict(xgb.DMatrix(train)), labels\n",
    "    )\n",
    "    train_scores.append(train_score)\n",
    "    score = sc.multi_log_loss(predictions, answers)\n",
    "    scores.append(score)\n",
    "\n",
    "    print('cv train score', train_score)\n",
    "    print('cv score:', score)\n",
    "    \n",
    "print('train score:', np.mean(train_scores))\n",
    "print('score:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train_orig\n",
    "test = test_orig\n",
    "\n",
    "train = base_features(train_orig)\n",
    "test = base_features(test_orig)\n",
    "train, test = location_features(train, test, cutoff=5)\n",
    "train, test = dangerous_location(train, test)\n",
    "train, test = event_severity(train, test)\n",
    "\n",
    "labels = train.fault_severity.values\n",
    "ids = test['id']\n",
    "\n",
    "train.drop(['id', 'fault_severity', 'location'], axis=1, inplace=True)\n",
    "test.drop(['id', 'location'], axis=1, inplace=True)\n",
    "\n",
    "train = train.fillna(0)\n",
    "train = train.astype(float)\n",
    "\n",
    "test = test.fillna(0)\n",
    "test = test.astype(float)\n",
    "\n",
    "ch2 = GenericUnivariateSelect(score_func=chi2,\n",
    "                              mode='percentile',\n",
    "                              param=80)\n",
    "train = ch2.fit_transform(train, labels)\n",
    "test = ch2.transform(test)\n",
    "\n",
    "model, predictions = xgboost_model(train, labels, test)\n",
    "\n",
    "train_score = multi_log_loss(\n",
    "    model.predict(xgb.DMatrix(train)), labels\n",
    ")\n",
    "print('train score', train_score)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'predict_0': predictions[:, 0],\n",
    "        'predict_1': predictions[:, 1],\n",
    "        'predict_2': predictions[:, 2]\n",
    "    })\n",
    "predictions_df.to_csv('{}_v{}.csv'.format('xgb', version),\n",
    "                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# library(data.table)\n",
    "# library(magrittr)\n",
    "# library(stringr)\n",
    "# library(doMC)\n",
    "# library(caret)\n",
    "\n",
    "# #\n",
    "# remove0cols <- T\n",
    "# writeFiles <- T\n",
    "# computeFeatures <- T\n",
    "# computeManualInteractions <- T\n",
    "# groupCardinalities <- T\n",
    "\n",
    "# log_feature_rarityThreshold <- 50\n",
    "# event_type_rarity_threshold <- 5\n",
    "# #\n",
    "\n",
    "# source('/Users/cavagnolo/testing/delattore/utils.R')\n",
    "\n",
    "# train <-\n",
    "#   fread(\"/Users/cavagnolo/testing/delattore/data/train.csv\") %>% setkey(id)\n",
    "# test <-\n",
    "#   fread(\"/Users/cavagnolo/testing/delattore/data/test.csv\") %>% setkey(id)\n",
    "# log_feature <-\n",
    "#   fread(\"/Users/cavagnolo/testing/delattore/data/log_feature.csv\") %>% setkey(id)\n",
    "# event_type <-\n",
    "#   fread(\"/Users/cavagnolo/testing/delattore/data/event_type.csv\") %>% setkey(id)\n",
    "# resource_type <-\n",
    "#   fread(\"/Users/cavagnolo/testing/delattore/data/resource_type.csv\") %>% setkey(id)\n",
    "# severity_type <-\n",
    "#   fread(\"/Users/cavagnolo/testing/delattore/data/severity_type.csv\") %>% setkey(id)\n",
    "\n",
    "\n",
    "# log_feature[, \":=\"(numlf = makeNumeric(log_feature), log_feature = makeReadable(log_feature))]\n",
    "# event_type[, \":=\"(numet = makeNumeric(event_type), event_type = makeReadable(event_type))]\n",
    "# resource_type[, \":=\"(resource_type = makeReadable(resource_type))]\n",
    "# severity_type[, \":=\"(numst = makeNumeric(severity_type), severity_type = makeReadable(severity_type))]\n",
    "\n",
    "\n",
    "# train[, \":=\"(numloc = makeNumeric(location),\n",
    "#              location = makeReadable(location))]\n",
    "# test[, \":=\"(numloc = makeNumeric(location),\n",
    "#             location = makeReadable(location), fault_severity = -1)]\n",
    "\n",
    "# total <- rbind(train, test)%>%setkey(\"id\")\n",
    "\n",
    "# if(groupCardinalities){\n",
    "#   #LF reduction\n",
    "#   rare_lf <- log_feature[total][,.N, by=log_feature][N<=log_feature_rarityThreshold][,log_feature]\n",
    "#   log_feature$log_feature[log_feature$log_feature%in%rare_lf] <- \"rare_lf\"\n",
    "  \n",
    "#   rare_et <- event_type[total][,.N, by=event_type][N<=event_type_rarity_threshold][,event_type]\n",
    "#   event_type$event_type[event_type$event_type%in%rare_et] <- \"rare_et\"\n",
    "#   rm(list=c(\"rare_lf\", \"rare_et\"))\n",
    "# }\n",
    "\n",
    "# if(computeFeatures){\n",
    "\n",
    "#   t1 <- log_feature[total][,.(\n",
    "#     loc_nid = uniqueN(id),\n",
    "#     loc_nlf = uniqueN(log_feature),\n",
    "#     loc_sumvol = sum(volume),\n",
    "#     loc_avgvol = mean(volume),\n",
    "#     loc_sdvol = ifelse(is.na(sd(volume)), 0, sd(volume)),\n",
    "#     loc_logvol = log(sum(volume)),\n",
    "#     loc_vollog = sum(log(volume+1)),\n",
    "#     loc_sqrtvol = sqrt(sum(volume)),\n",
    "#     loc_volsqrt = sum(sqrt(volume))), keyby=location]\n",
    "  \n",
    "#   t2 <- resource_type[total][, .(loc_nrt = uniqueN(resource_type)), keyby=location]\n",
    "#   t3 <- event_type[total][, .(loc_net = uniqueN(event_type)), keyby=location]\n",
    "#   t4 <- severity_type[total][, .(loc_nst = uniqueN(severity_type)), keyby=location]\n",
    "  \n",
    "#   joined_total <- resource_type[event_type][severity_type][total]\n",
    "#   # nombres de combinaisons différentes de resource_type/event_type par location\n",
    "#   t5 <- joined_total[,etrtcomb := paste(resource_type, event_type, sep = \"x\")][, .(loc_etrtcomb = uniqueN(etrtcomb)), keyby=location]\n",
    "#   t6 <- joined_total[,etstcomb := paste(severity_type, event_type, sep = \"x\")][, .(loc_etstcomb = uniqueN(etstcomb)), keyby=location]\n",
    "#   t7 <- joined_total[,rtstcomb := paste(severity_type, resource_type, sep = \"x\")][, .(loc_rtstcomb = uniqueN(rtstcomb)), keyby=location]\n",
    "  \n",
    "#   location_info_total <- t1[t2][t3][t4][t5][t6][t7]\n",
    "  \n",
    "#   # tests sur les log features en temps que numérique moyenne/id, somme/id, etc\n",
    "#   lf_info <- log_feature[total][,.(\n",
    "#     sumlf = sum(numlf), \n",
    "#     avglf = mean(numlf), \n",
    "#     sdlf = ifelse(is.na(sd(numlf)), 0, sd(numlf)),\n",
    "#     minlf = min(numlf),\n",
    "#     maxlf = max(numlf)),by=id]\n",
    "  \n",
    "#   # moyenne numerique de l'event_type * severity_type\n",
    "#   etst <- event_type[, .(\n",
    "#     id, \n",
    "#     minumet = min(numet), \n",
    "#     maxnumet = max(numet), \n",
    "#     avgnumet = mean(numet)), by=id][severity_type][total][,.(et.st = avgnumet*numst),by=id]\n",
    "  \n",
    "#   total <- total[lf_info][etst]\n",
    "#   total <- merge(total, location_info_total, by = \"location\")\n",
    "# }\n",
    "# setkeyv(total[,\":=\"(location=NULL)], c(\"id\", \"fault_severity\"))\n",
    "\n",
    "# # Y a-t-il des NA dans le jeu de train, si oui on droppe les lignes\n",
    "# na.ids <- log_feature[total][fault_severity != -1][is.na(log_feature)][,id]\n",
    "# if(length(na.ids) > 0)\n",
    "#   total <- total[-na.ids]\n",
    "# ####################################\n",
    "# # dcast des data pour avoir une ligne par id\n",
    "\n",
    "# # total_lf_volume  <- dcast(\n",
    "# #   log_feature[total],\n",
    "# #   id + fault_severity ~ log_feature,\n",
    "# #   value.var = list(\"volume\", \"log_feature\"),\n",
    "# #   fun = list(sum, length)\n",
    "# # )\n",
    "\n",
    "# # on garde uniquement le volume\n",
    "# total_lf_volume  <- dcast(\n",
    "#   log_feature[total],\n",
    "#   id + fault_severity ~ log_feature,\n",
    "#   value.var = \"volume\",\n",
    "#   fun = sum\n",
    "# )\n",
    "\n",
    "# total_et <-\n",
    "#   dcast(event_type[total], id + fault_severity ~ event_type, value.var = \"event_type\", fun = length)\n",
    "# total_rt <-\n",
    "#   dcast(\n",
    "#     resource_type[total], id + fault_severity ~ resource_type, value.var = \"resource_type\", fun = length\n",
    "#   )\n",
    "# total_st <-\n",
    "#   dcast(\n",
    "#     severity_type[total], id + fault_severity ~ severity_type, value.var = \"severity_type\", fun = length\n",
    "#   )\n",
    "\n",
    "# total.wide <-\n",
    "#   total[total_lf_volume][total_et][total_rt][total_st]\n",
    "\n",
    "# ######################################################################################\n",
    "# #calculs d'intercations entre event_type, resource_types, log_features\n",
    "# if(computeManualInteractions){\n",
    "#   total.wide[,\":=\"(\n",
    "\n",
    "#     som_vol_test = (f203 * f312 * f232 * f170),\n",
    "    \n",
    "#     som_vol_feat = (f82 + f203 + f71 + f193 + f80),\n",
    "#     som_vol_feat_c0 = (f313 + f233 + f315),\n",
    "#     som_vol_feat_c1 = (f82 + f203 + f170),\n",
    "#     som_vol_feat_c2 = (f71 + f193 + f80) ) ]\n",
    "# }\n",
    "\n",
    "# ######################################################################################\n",
    "# train.wide <- total.wide[fault_severity != -1,-\"id\", with = FALSE]\n",
    "# test.wide <- total.wide[fault_severity == -1,-\"id\", with = FALSE]\n",
    "\n",
    "# # write files with train and test\n",
    "# if(writeFiles){\n",
    "#   writeLines(\"Writing train.csv and test.csv...\")\n",
    "#   write.csv(train.wide, paste(sep = \"-\", \"train.csv\"), row.names = F, quote = F)\n",
    "#   write.csv(test.wide[,.SD, .SDcols = -\"fault_severity\"], paste(sep = \"-\", \"test.csv\"), row.names = F, quote = F)\n",
    "#   writeLines(\"...done\")}\n",
    "\n",
    "# xtrain <- as.matrix(train.wide[,-\"fault_severity\", with = F])\n",
    "# names(xtrain) <- setdiff(names(train.wide), \"fault_severity\")\n",
    "# ytrain <- train.wide$fault_severity\n",
    "\n",
    "# xtest <-  as.matrix(test.wide[,-\"fault_severity\", with = F])\n",
    "# names(xtest) <- setdiff(names(test.wide), \"fault_severity\")\n",
    "# test.id <- test$id\n",
    "\n",
    "# # séparer le train set en 2 pour le blending\n",
    "# # folds <- createFolds(train.wide$fault_severity, k = 3)\n",
    "# # 80% train + 20% pour valider le stacking\n",
    "# #if(onFold) folds <- createDataPartition(train.wide$fault_severity, p = 0.8)\n",
    "\n",
    "\n",
    "# writeLines(\"Cleaning up...\")\n",
    "# rm(list=c(\"na.ids\", \"total.wide\", \"total_et\", \"total_rt\", \"total_st\", \n",
    "#           \"total_lf_volume\", \"train.wide\", \"test.wide\", \"total\",\n",
    "#           \"location_info_total\",\"joined_total\", \"lf_info\", paste0(\"t\", 1:7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set.seed(123456)\n",
    "\n",
    "# if(onFold){\n",
    "#   writeLines(paste(\"Starting xgboost on a fold1....\",nrow(xtrain[folds$Resample1,]), \" lines\" ))\n",
    "#   dtrain <- xgb.DMatrix(data = xtrain[folds$Resample1,], label = ytrain[folds$Resample1])\n",
    "#   xtest <- xtrain[-folds$Resample1,]\n",
    "#   test.id <- test.id[-folds$Resample1]\n",
    "# }else{\n",
    "#   writeLines(\"Starting xgboost on full train...\")\n",
    "#   dtrain <- xgb.DMatrix(data = xtrain, label = ytrain)\n",
    "# }\n",
    "\n",
    "# registerDoMC(cores = 6)\n",
    "\n",
    "# xgparams.tree <- list(\n",
    "#   objective = \"multi:softprob\",\n",
    "#   num_class = 3,\n",
    "#   colsample_bytree = 0.3,\n",
    "#   max.depth = 8,\n",
    "#   eta = 0.05\n",
    "# )\n",
    "\n",
    "# xgboost.first <- xgb.cv(\n",
    "#   data = dtrain,\n",
    "#   params = xgparams.tree,\n",
    "#   nrounds = 500,\n",
    "#   nfold = 10,\n",
    "#   metrics = \"mlogloss\",\n",
    "#   verbose = verboseXgboost,\n",
    "#   print.every.n = 200\n",
    "# )\n",
    "\n",
    "# cat(\"xVal mlogloss : \", min(xgboost.first$test.mlogloss.mean),\"\\n\")\n",
    "\n",
    "# if(CVonly)\n",
    "# {\n",
    "#   pred.loop <- matrix(nrow = nrow(xtest)*3, ncol = 10)\n",
    "#   for(index in 1:10)\n",
    "#   {\n",
    "#     set.seed(28021980+index)\n",
    "#     xgboost.model <- xgboost(\n",
    "#       data = dtrain,\n",
    "#       params = xgparams.tree,\n",
    "#       nrounds = which.min(xgboost.first$test.mlogloss.mean),\n",
    "#       verbose = verboseXgboost\n",
    "#     )\n",
    "#     pred.loop[,index] <- xgboost::predict(xgboost.model, xtest)\n",
    "#   }\n",
    "\n",
    "#   pred.xgboost <- matrix(apply(pred.loop, MARGIN = 1, mean), ncol = 3, byrow = T)\n",
    "  \n",
    "\n",
    "# if(importance){\n",
    "#   writeLines(\"Computing importance...\")\n",
    "#   imp <- xgb.importance(feature_names = names(xtrain), model = xgboost.model)\n",
    "#   }\n",
    "  \n",
    "#   if (genererSubmission) {\n",
    "#     cat(\"Generating Submission...\\n\")\n",
    "#     output.xgboost <- data.frame(\n",
    "#       id = test.id,\n",
    "#       predict_0 = pred.xgboost[,1],\n",
    "#       predict_1 = pred.xgboost[,2],\n",
    "#       predict_2 = pred.xgboost[,3]\n",
    "#     )\n",
    "#     write.csv(output.xgboost, paste(sep = \"-\", format(Sys.time(), \"%Y%m%d.%H%M\"), \"submission.csv\"), row.names = F, quote = F)\n",
    "#   }\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
