{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_dataset.csv')\n",
    "test = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insample_evaluates = []\n",
    "outsample_evaluates = []\n",
    "com_cats = []\n",
    "for rid, com_range in enumerate(full_ranges[1:]):\n",
    "    print(rid)\n",
    "    col_ids = list_sum(x[1] for x in com_range)\n",
    "    com_cat = string_sum(x[0] for x in com_range)\n",
    "    x = tf.placeholder('float', shape=[None, len(col_ids)])\n",
    "    y_ = tf.placeholder('float', shape=[None, 3])\n",
    "    l2 = tf.placeholder('float')\n",
    "    hidden1 = 2000\n",
    "    hidden2 = 100\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([len(col_ids), hidden1],\n",
    "                                stddev=1.0 / np.sqrt(float(len(col_ids)))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1]),\n",
    "                             name='biases')\n",
    "        hidden1_tensor = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "        reg_hidden1 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1, hidden2],\n",
    "                                stddev=1.0 / np.sqrt(float(hidden1))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2]),\n",
    "                             name='biases')\n",
    "        hidden2_tensor = tf.nn.relu(tf.matmul(hidden1_tensor, weights) + biases)\n",
    "        reg_hidden2 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2, 3],\n",
    "                                stddev=1.0 / np.sqrt(float(hidden2))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([3]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2_tensor, weights) + biases\n",
    "        reg_hidden3 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    sess.run(tf.initialize_all_variables()) \n",
    "    y = tf.nn.softmax(logits)\n",
    "    cross_entropy = - tf.reduce_mean(y_ * tf.log(y) + 1e-15) + l2 * (reg_hidden1 + reg_hidden2 + reg_hidden3)\n",
    "    train_step = tf.train.GradientDescentOptimizer(.001).minimize(cross_entropy)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        train.iloc[:, col_ids],\n",
    "        train.loc[:, 'fault_severity'],\n",
    "        test_size=0.3333,\n",
    "        random_state=100383)\n",
    "    labels = tf.expand_dims(np.asarray(train_y.values, dtype='int32'), 1)\n",
    "    indices = tf.expand_dims(tf.range(0, train_x.shape[0]), 1)\n",
    "    concated = tf.concat(1, [indices, labels])\n",
    "    train_onehot_labels = tf.sparse_to_dense(\n",
    "        concated, tf.pack([train_x.shape[0], sample.shape[1]-1]), 1.0, 0.0).eval()\n",
    "    for i in range(train_x.shape[0]//batch_size):\n",
    "        train_step.run(\n",
    "            feed_dict={\n",
    "                x: train_x.iloc[batch_size*i:batch_size*(i+1), :].values,\n",
    "                y_: train_onehot_labels[batch_size*i:batch_size*(i+1), :],\n",
    "                l2: 0})\n",
    "    labels = tf.expand_dims(np.asarray(valid_y.values, dtype='int32'), 1)\n",
    "    indices = tf.expand_dims(tf.range(0, valid_x.shape[0]), 1)\n",
    "    concated = tf.concat(1, [indices, labels])\n",
    "    valid_onehot_labels = tf.sparse_to_dense(\n",
    "        concated, tf.pack([valid_x.shape[0], sample.shape[1]-1]), 1.0, 0.0).eval()\n",
    "    evaluate = - tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y), 1))\n",
    "    insample_evaluate = evaluate.eval(feed_dict={x: train_x, y_: train_onehot_labels})\n",
    "    insample_evaluates.append(insample_evaluate)\n",
    "    outsample_evaluate = evaluate.eval(feed_dict={x: valid_x, y_: valid_onehot_labels})\n",
    "    outsample_evaluates.append(outsample_evaluate)\n",
    "    com_cats.append(com_cat)\n",
    "res = pd.DataFrame(\n",
    "    {'insample_evaluates': insample_evaluates,\n",
    "     'outsample_evaluates': outsample_evaluates}, index=com_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(111)\n",
    "res.loc[:, 'insample_evaluates'].plot(kind='line', ax=ax, rot=90, marker='*')\n",
    "res.loc[:, 'outsample_evaluates'].plot(kind='line', ax=ax, color='r', rot=90, marker='*')\n",
    "plt.xticks(range(len(res.index)), res.index)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(111)\n",
    "res.loc[:, 'insample_evaluates'].plot(kind='line', ax=ax, rot=90, marker='*')\n",
    "res.loc[:, 'outsample_evaluates'].plot(kind='line', ax=ax, color='r', rot=90, marker='*')\n",
    "plt.xticks(range(len(res.index)), res.index)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insample_evaluates = []\n",
    "outsample_evaluates = []\n",
    "regs = [0.01, .1, .2, .3, .4]\n",
    "for rid, reg in enumerate(regs):\n",
    "    print(rid)\n",
    "    col_ids = list_sum(x[1] for x in ranges[2:4])\n",
    "    x = tf.placeholder('float', shape=[None, len(col_ids)])\n",
    "    y_ = tf.placeholder('float', shape=[None, 3])\n",
    "    l2 = tf.placeholder('float')\n",
    "    hidden1 = 2000\n",
    "    hidden2 = 100\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([len(col_ids), hidden1],\n",
    "                                stddev=1.0 / np.sqrt(float(len(col_ids)))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1]),\n",
    "                             name='biases')\n",
    "        hidden1_tensor = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "        reg_hidden1 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1, hidden2],\n",
    "                                stddev=1.0 / np.sqrt(float(hidden1))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2]),\n",
    "                             name='biases')\n",
    "        hidden2_tensor = tf.nn.relu(tf.matmul(hidden1_tensor, weights) + biases)\n",
    "        reg_hidden2 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2, 3],\n",
    "                                stddev=1.0 / np.sqrt(float(hidden2))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([3]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2_tensor, weights) + biases\n",
    "        reg_hidden3 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "    sess.run(tf.initialize_all_variables()) \n",
    "    y = tf.nn.softmax(logits)\n",
    "    cross_entropy = - tf.reduce_mean(y_ * tf.log(y) + 1e-15) + l2 * (reg_hidden1 + reg_hidden2 + reg_hidden3)\n",
    "    train_step = tf.train.GradientDescentOptimizer(.001).minimize(cross_entropy)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        train.iloc[:, col_ids],\n",
    "        train.loc[:, 'fault_severity'],\n",
    "        test_size=0.3333)\n",
    "    labels = tf.expand_dims(np.asarray(train_y.values, dtype='int32'), 1)\n",
    "    indices = tf.expand_dims(tf.range(0, train_x.shape[0]), 1)\n",
    "    concated = tf.concat(1, [indices, labels])\n",
    "    train_onehot_labels = tf.sparse_to_dense(\n",
    "        concated, tf.pack([train_x.shape[0], sample.shape[1]-1]), 1.0, 0.0).eval()\n",
    "    for i in range(train_x.shape[0]//batch_size):\n",
    "        train_step.run(\n",
    "            feed_dict={\n",
    "                x: train_x.iloc[batch_size*i:batch_size*(i+1), :].values,\n",
    "                y_: train_onehot_labels[batch_size*i:batch_size*(i+1), :],\n",
    "                l2: 0})\n",
    "    labels = tf.expand_dims(np.asarray(valid_y.values, dtype='int32'), 1)\n",
    "    indices = tf.expand_dims(tf.range(0, valid_x.shape[0]), 1)\n",
    "    concated = tf.concat(1, [indices, labels])\n",
    "    valid_onehot_labels = tf.sparse_to_dense(\n",
    "        concated, tf.pack([valid_x.shape[0], sample.shape[1]-1]), 1.0, 0.0).eval()\n",
    "    evaluate = - tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y), 1))\n",
    "    insample_evaluate = evaluate.eval(feed_dict={x: train_x, y_: train_onehot_labels})\n",
    "    insample_evaluates.append(insample_evaluate)\n",
    "    outsample_evaluate = evaluate.eval(feed_dict={x: valid_x, y_: valid_onehot_labels})\n",
    "    outsample_evaluates.append(outsample_evaluate)\n",
    "res = pd.DataFrame(\n",
    "    {'insample_evaluates': insample_evaluates,\n",
    "     'outsample_evaluates': outsample_evaluates}, index=regs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_ids = list_sum(x[1] for x in ranges[2:4])\n",
    "x = tf.placeholder('float', shape=[None, len(col_ids)])\n",
    "y_ = tf.placeholder('float', shape=[None, 3])\n",
    "l2 = tf.placeholder('float')\n",
    "hidden1 = 2000\n",
    "hidden2 = 100\n",
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([len(col_ids), hidden1],\n",
    "                            stddev=1.0 / np.sqrt(float(len(col_ids)))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1]),\n",
    "                         name='biases')\n",
    "    hidden1_tensor = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "    reg_hidden1 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1, hidden2],\n",
    "                            stddev=1.0 / np.sqrt(float(hidden1))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2]),\n",
    "                         name='biases')\n",
    "    hidden2_tensor = tf.nn.relu(tf.matmul(hidden1_tensor, weights) + biases)\n",
    "    reg_hidden2 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2, 3],\n",
    "                            stddev=1.0 / np.sqrt(float(hidden2))),\n",
    "        name='weights')\n",
    "    biases = tf.Variable(tf.zeros([3]),\n",
    "                         name='biases')\n",
    "    logits = tf.matmul(hidden2_tensor, weights) + biases\n",
    "    reg_hidden3 = tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "sess.run(tf.initialize_all_variables()) \n",
    "y = tf.nn.softmax(logits)\n",
    "cross_entropy = - tf.reduce_mean(y_ * tf.log(y) + 1e-15) + l2 * (reg_hidden1 + reg_hidden2 + reg_hidden3)\n",
    "train_step = tf.train.GradientDescentOptimizer(.001).minimize(cross_entropy)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "        train.iloc[:, col_ids],\n",
    "        train.loc[:, 'fault_severity'],\n",
    "        test_size=0.3333)\n",
    "labels = tf.expand_dims(np.asarray(train.loc[:, 'fault_severity'].values, dtype='int32'), 1)\n",
    "indices = tf.expand_dims(tf.range(0, train.shape[0]), 1)\n",
    "concated = tf.concat(1, [indices, labels])\n",
    "train_onehot_labels = tf.sparse_to_dense(\n",
    "    concated, tf.pack([train.shape[0], sample.shape[1]-1]), 1.0, 0.0).eval()\n",
    "for i in range(train_x.shape[0]//batch_size):\n",
    "    train_step.run(\n",
    "        feed_dict={\n",
    "            x: train.iloc[batch_size*i:batch_size*(i+1), col_ids].values,\n",
    "            y_: train_onehot_labels[batch_size*i:batch_size*(i+1), :],\n",
    "            l2: 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ranges = [\n",
    "    ('resource', range(test_resource_start, test_severity_start)),\n",
    "    ('severity', range(test_severity_start, test_volume_start))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_col_ids = list_sum(x[1] for x in test_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = y.eval(feed_dict={x: test.iloc[:, test_col_ids]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11171, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = pd.DataFrame({\n",
    "    'id': sample.loc[:, 'id'].values, \n",
    "    'predict_0': prob[:, 0],\n",
    "    'predict_1': prob[:, 1],\n",
    "    'predict_2': prob[:, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predict_0</th>\n",
       "      <th>predict_1</th>\n",
       "      <th>predict_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11066</td>\n",
       "      <td>0.618390</td>\n",
       "      <td>0.223752</td>\n",
       "      <td>0.157857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18000</td>\n",
       "      <td>0.485847</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.213340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16964</td>\n",
       "      <td>0.618390</td>\n",
       "      <td>0.223752</td>\n",
       "      <td>0.157857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4795</td>\n",
       "      <td>0.435015</td>\n",
       "      <td>0.324048</td>\n",
       "      <td>0.240937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3392</td>\n",
       "      <td>0.530771</td>\n",
       "      <td>0.274856</td>\n",
       "      <td>0.194373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  predict_0  predict_1  predict_2\n",
       "0  11066   0.618390   0.223752   0.157857\n",
       "1  18000   0.485847   0.300813   0.213340\n",
       "2  16964   0.618390   0.223752   0.157857\n",
       "3   4795   0.435015   0.324048   0.240937\n",
       "4   3392   0.530771   0.274856   0.194373"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.to_csv('neural_network.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
